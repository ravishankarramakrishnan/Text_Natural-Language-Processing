{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>length</th>\n",
       "      <th>punct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  length  punct\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     111      9\n",
       "1   ham                      Ok lar... Joking wif u oni...      29      6\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155      6\n",
       "3   ham  U dun say so early hor... U c already then say...      49      6\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...      61      2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing and Loading the Data\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('smsspamcollection.tsv', sep='\\t')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label      0\n",
       "message    0\n",
       "length     0\n",
       "punct      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for Missing values\n",
    "\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ham' 'spam']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ham     4825\n",
       "spam     747\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the Output Label\n",
    "\n",
    "print(df['label'].unique())\n",
    "\n",
    "# Check the Counts of the Data\n",
    "\n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAWUUlEQVR4nO3df5BdZZ3n8feXEMmoSDQ0VOyOdhzjbIAuwtgmsFIlKgsBBwMKM+CoiTJGrUCBjhqYsgpWlyrFhYzIbiQYhjDF8GOBWcKPdQb5pVTxq8NEkpBxiNArbVIkE0gmiLAkfPePexKbcDt9b/e9fbtPv19VXffc5zzn9Ld5uJ978txzz4nMRJJULvu1ugBJUuMZ7pJUQoa7JJWQ4S5JJWS4S1IJGe6SVEL7t7oAgIMPPjg7OztbXYYkjSmrVq3698xsq7ZuVIR7Z2cnPT09rS5DksaUiPi/A61zWkaSSshwl6QSqjncI2JCRPxLRNxZPJ8eEY9GxNMRcVNEvKVoP6B4vqFY39mc0iVJA6lnzv08YD3wjuL594ElmXljRPwYOBtYWjy+mJnvj4gzi35/0cCaJY1Tr732Gn19fbzyyiutLmVETZo0iY6ODiZOnFjzNjWFe0R0AJ8ALgG+HhEBfAz4TNFlBXAxlXCfVywD3AJcGRGRXqFM0jD19fVx4IEH0tnZSSWGyi8z2bp1K319fUyfPr3m7Wqdlvlb4FvA68XzKcC2zNxZPO8D2ovlduC5oqidwPaivyQNyyuvvMKUKVPGTbADRARTpkyp+18rg4Z7RPwZsDkzV/VvrtI1a1jXf78LI6InInq2bNlSU7GSNJ6Cfbeh/M21HLl/GPhkRPQCN1KZjvlbYHJE7J7W6QA2Fst9wLSioP2Bg4AX9t5pZi7LzO7M7G5rq3oOviSNOr29vRxxxBGtLmNQg865Z+aFwIUAEXEc8I3M/MuI+F/A6VQCfz5we7HJyuL5w8X6+5xvV9md8qOHqrbfce6xI1zJ+DLQf/ehKtN4Dec898VUPlzdQGVOfXnRvhyYUrR/HbhgeCVK0uiya9cuvvSlL3H44Ydzwgkn8Pvf/56rr76aD33oQxx55JF8+tOf5uWXXwZgwYIFfPWrX+WjH/0o73vf+3jwwQf54he/yMyZM1mwYEHTaqwr3DPzgcz8s2L5mcycnZnvz8wzMvPVov2V4vn7i/XPNKNwSWqVp59+mkWLFrFu3TomT57Mrbfeyqc+9Skef/xxfvnLXzJz5kyWL1++p/+LL77Ifffdx5IlSzjllFP42te+xrp161izZg2rV69uSo2j4toyUlk5XVNO06dPZ9asWQB88IMfpLe3l7Vr1/Ltb3+bbdu28dJLL3HiiSfu6X/KKacQEXR1dXHooYfS1dUFwOGHH05vb++efTWSlx+QpDodcMABe5YnTJjAzp07WbBgAVdeeSVr1qzhoosuesOpi7v777fffm/Ydr/99mPnzp00g+EuSQ2wY8cOpk6dymuvvcb111/f6nKclpGkRvjud7/LnDlzeO9730tXVxc7duxoaT0xGs5S7O7uTq/nrrGs3lPynHMfmvXr1zNz5sxWl9ES1f72iFiVmd3V+jstI0klZLhLUgkZ7pJUQoa7JJWQ4S5JJWS4S1IJGe6SVEJ+iUnS2HXVRxq7vy8/2Nj9tZBH7pJUo9/97nd84hOf4Mgjj+SII47gpptuorOzk8WLFzN79mxmz57Nhg0bALjjjjuYM2cORx11FMcffzzPP/88ABdffDHz58/nhBNOoLOzk9tuu41vfetbdHV1MXfuXF577bWG1OqRu1SHRt8cQmPLT3/6U9797ndz1113AbB9+3YWL17MO97xDh577DGuu+46zj//fO68806OPfZYHnnkESKCn/zkJ1x66aVcdtllAPz617/m/vvv56mnnuKYY47h1ltv5dJLL+W0007jrrvu4tRTTx12rR65S1KNurq6+NnPfsbixYv5xS9+wUEHHQTAWWedtefx4YcfBqCvr48TTzyRrq4ufvCDH7Bu3bo9+znppJOYOHEiXV1d7Nq1i7lz5+7Zf29vb0NqNdwlqUYf+MAHWLVqFV1dXVx44YV85zvfAd54A+vdy+eeey7nnHMOa9as4aqrrhrwEsATJ07cs00jLwFsuEtSjTZu3Mhb3/pWPvvZz/KNb3yDJ554AoCbbrppz+MxxxwDVKZs2tvbAVixYsWI1zronHtETAJ+DhxQ9L8lMy+KiGuBjwDbi64LMnN1VN6CfgicDLxctD/RjOIlaSStWbOGb37zm3uOuJcuXcrpp5/Oq6++ypw5c3j99de54YYbgMoHp2eccQbt7e0cffTRPPvssyNa66CX/C3C+m2Z+VJETAQeAs4DvgLcmZm37NX/ZOBcKuE+B/hhZs7Z1+/wkr8aKxr1gaqX/B2a0XjJ387OTnp6ejj44IOb+nsafsnfrHipeDqx+NnXO8I84Lpiu0eAyRExtabqJUkNUdOce0RMiIjVwGbgnsx8tFh1SUQ8GRFLImL3jQHbgef6bd5XtO29z4UR0RMRPVu2bBnGnyBJrdPb29v0o/ahqCncM3NXZs4COoDZEXEEcCHwn4APAe8CFhfdo9ouquxzWWZ2Z2Z3W1vbkIqXJFVX19kymbkNeACYm5mbiqmXV4G/A2YX3fqAaf026wA2NqBWSWI03Bp0pA3lbx403COiLSImF8t/BBwP/OvuefTiA9dTgbXFJiuBz0fF0cD2zNxUd2WStJdJkyaxdevWcRXwmcnWrVuZNGlSXdvVcvmBqcCKiJhA5c3g5sy8MyLui4g2KtMwq6mcPQNwN5UzZTZQORXyC3VVJEkD6OjooK+vj/H2Od2kSZPo6Oioa5tBwz0znwSOqtL+sQH6J7CoriokqQYTJ05k+vTprS5jTPAbqpJUQl4VUuPWQF9I8gtGKgOP3CWphAx3SSohp2WkvXhDDpWBR+6SVEKGuySVkOEuSSVkuEtSCRnuklRChrsklZDhLkklZLhLUgn5JSaVnl9K0nhkuEstMJQ3HC9opno4LSNJJWS4S1IJ1XIP1UkR8VhE/DIi1kXEfy3ap0fEoxHxdETcFBFvKdoPKJ5vKNZ3NvdPkCTtrZYj91eBj2XmkcAsYG5x4+vvA0sycwbwInB20f9s4MXMfD+wpOgnSRpBtdxDNYGXiqcTi58EPgZ8pmhfAVwMLAXmFcsAtwBXRkTkeLpdudQE3jlK9ahpzj0iJkTEamAzcA/wa2BbZu4suvQB7cVyO/AcQLF+OzClyj4XRkRPRPSMtzuZS1Kz1RTumbkrM2cBHcBsYGa1bsVj7GNd/30uy8zuzOxua2urtV5JUg3qOlsmM7cBDwBHA5MjYve0TgewsVjuA6YBFOsPAl5oRLGSpNrUcrZMW0RMLpb/CDgeWA/cD5xedJsP3F4sryyeU6y/z/l2SRpZtXxDdSqwIiImUHkzuDkz74yIp4AbI+K/Af8CLC/6Lwf+PiI2UDliP7MJdUuS9qGWs2WeBI6q0v4Mlfn3vdtfAc5oSHWSRrerPlK9/csPjmwdehO/oSpJJWS4S1IJGe6SVEKGuySVkOEuSSXkzTqkMc5rzqgaj9wlqYQMd0kqIcNdkkrIcJekEjLcJamEDHdJKiHDXZJKyPPcJQ1qwHPp3zLChahmHrlLUgkZ7pJUQk7LSBrU5dvOq77ikLePbCGqWS33UJ0WEfdHxPqIWBcR5xXtF0fEbyNidfFzcr9tLoyIDRHxq4g4sZl/gCTpzWo5ct8J/HVmPhERBwKrIuKeYt2SzPzv/TtHxGFU7pt6OPBu4GcR8YHM3NXIwiVJAxv0yD0zN2XmE8XyDmA90L6PTeYBN2bmq5n5LLCBKvdalSQ1T10fqEZEJ5WbZT9aNJ0TEU9GxDUR8c6irR14rt9mfVR5M4iIhRHRExE9W7ZsqbtwSdLAag73iHg7cCtwfmb+B7AU+GNgFrAJuGx31yqb55saMpdlZndmdre1tdVduCRpYDWFe0RMpBLs12fmbQCZ+Xxm7srM14Gr+cPUSx8wrd/mHcDGxpUsSRrMoB+oRkQAy4H1mXl5v/apmbmpeHoasLZYXgn8Q0RcTuUD1RnAYw2tWlJTDPRN1Murtmo0q+VsmQ8DnwPWRMTqou1vgLMiYhaVKZde4MsAmbkuIm4GnqJyps0iz5SRpJE1aLhn5kNUn0e/ex/bXAJcMoy6JEnD4OUHJKmEDHdJKiHDXZJKyHCXpBIy3CWphAx3SSohw12SSshwl6QSMtwlqYS8zZ6kIXt680tV22eMcB16M4/cJamEDHdJKiHDXZJKyHCXpBLyA1WppAa68cYd5x47wpWoFQx3aZwZKPRVLk7LSFIJDRruETEtIu6PiPURsS4iziva3xUR90TE08XjO4v2iIgrImJDRDwZEX/a7D9CkvRGtRy57wT+OjNnAkcDiyLiMOAC4N7MnAHcWzwHOInKdxhmAAuBpQ2vWpK0T4OGe2ZuyswniuUdwHqgHZgHrCi6rQBOLZbnAddlxSPA5IiY2vDKJUkDqmvOPSI6gaOAR4FDM3MTVN4AgEOKbu3Ac/026yva9t7XwojoiYieLVu21F+5JGlANYd7RLwduBU4PzP/Y19dq7Tlmxoyl2Vmd2Z2t7W11VqGJKkGNYV7REykEuzXZ+ZtRfPzu6dbisfNRXsfMK3f5h3AxsaUK0mqRS1nywSwHFifmZf3W7USmF8szwdu79f++eKsmaOB7bunbyRJI6OWLzF9GPgcsCYiVhdtfwN8D7g5Is4GfgOcUay7GzgZ2AC8DHyhoRVLkgY1aLhn5kNUn0cH+HiV/gksGmZdkqRh8BuqklRChrsklZDhLkklZLhLUgkZ7pJUQoa7JJWQ4S5JJWS4S1IJGe6SVEKGuySVkOEuSSVkuEtSCRnuklRChrsklZDhLkklZLhLUgkZ7pJUQrXcQ/WaiNgcEWv7tV0cEb+NiNXFz8n91l0YERsi4lcRcWKzCpckDayWI/drgblV2pdk5qzi526AiDgMOBM4vNjmf0bEhEYVK0mqzaDhnpk/B16ocX/zgBsz89XMfJbKTbJnD6M+SdIQDGfO/ZyIeLKYtnln0dYOPNevT1/RJkkaQUMN96XAHwOzgE3AZUV7VOmb1XYQEQsjoicierZs2TLEMiRJ1Qwp3DPz+czclZmvA1fzh6mXPmBav64dwMYB9rEsM7szs7utrW0oZUiSBjCkcI+Iqf2engbsPpNmJXBmRBwQEdOBGcBjwytRklSv/QfrEBE3AMcBB0dEH3ARcFxEzKIy5dILfBkgM9dFxM3AU8BOYFFm7mpO6ZKkgQwa7pl5VpXm5fvofwlwyXCKkiQNj99QlaQSMtwlqYQMd0kqIcNdkkrIcJekEjLcJamEDHdJKiHDXZJKyHCXpBIy3CWphAx3SSohw12SSshwl6QSMtwlqYQMd0kqIcNdkkrIcJekEho03CPimojYHBFr+7W9KyLuiYini8d3Fu0REVdExIaIeDIi/rSZxUuSqqvlyP1aYO5ebRcA92bmDODe4jnASVRuij0DWAgsbUyZkqR6DBrumflz4IW9mucBK4rlFcCp/dqvy4pHgMkRMbVRxUqSajPUOfdDM3MTQPF4SNHeDjzXr19f0SZJGkGN/kA1qrRl1Y4RCyOiJyJ6tmzZ0uAyJGl8G2q4P797uqV43Fy09wHT+vXrADZW20FmLsvM7szsbmtrG2IZkqRqhhruK4H5xfJ84PZ+7Z8vzpo5Gti+e/pGkjRy9h+sQ0TcABwHHBwRfcBFwPeAmyPibOA3wBlF97uBk4ENwMvAF5pQsyRpEIOGe2aeNcCqj1fpm8Ci4RYlSRoev6EqSSVkuEtSCRnuklRChrsklZDhLkklZLhLUgkZ7pJUQoa7JJWQ4S5JJWS4S1IJGe6SVEKDXltG0vhx+bbzWl2CGsQjd0kqIcNdkkrIcJekEjLcJamEDHdJKqFhnS0TEb3ADmAXsDMzuyPiXcBNQCfQC/x5Zr44vDIlSfVoxJH7RzNzVmZ2F88vAO7NzBnAvcVzSdIIasa0zDxgRbG8Aji1Cb9DkrQPww33BP45IlZFxMKi7dDM3ARQPB4yzN8hSarTcL+h+uHM3BgRhwD3RMS/1rph8WawEOA973nPMMuQJPU3rHDPzI3F4+aI+EdgNvB8REzNzE0RMRXYPMC2y4BlAN3d3TmcOiSAU370UKtLGBNG4hIDA43FHece2/TfrYohT8tExNsi4sDdy8AJwFpgJTC/6DYfuH24RUqS6jOcI/dDgX+MiN37+YfM/GlEPA7cHBFnA78Bzhh+mZKkegw53DPzGeDIKu1bgY8PpyhJ0vD4DVVJKiHDXZJKyHCXpBIy3CWphAx3SSohw12SSshwl6QSMtwlqYSGe+EwacR5DZnxxevUDI3hLmnE7OuN2bBuLMNd0qjgv8gay3BXS/mClprDcJfGiIGuw/71yT+sq7/GB8Nd0pjkB637ZrhLY5xH6KrGcC+h0XhE49y6NLIMdw3pzWA0voGMRvXOk6u5xtP/t4b7CBtP/3NJreC/EiuaFu4RMRf4ITAB+Elmfq9Zv6vMWvmlD18krVGGOXT/xdJ6TQn3iJgA/A/gvwB9wOMRsTIzn2rG72u2shxtj0RYl/0NoQzBq9qN5dd+s47cZwMbiptoExE3AvOAMRnuap6hHOE16nzvVp4fPl7fJPb1d7fyqL7eg5JGHsQ0640iMrPxO404HZibmX9VPP8cMCczz+nXZyGwsHj6J8CvquzqIGD7IG0HA//eoNLrVa2+kdpPrdsM1m9f6wdaV8u4wPgcG8dl33zNDNw2lHF5b2a2VV2TmQ3/Ac6gMs+++/nngB8NYT/LBmsDeprxNwy1vpHaT63bDNZvX+sHWlfLuIzXsXFcRue4jIWxafS4NOt67n3AtH7PO4CNQ9jPHTW2tUqjahnKfmrdZrB++1o/0LrRPi7QurFxXPbN10ztv2dYmjUtsz/wb8DHgd8CjwOfycx1TfhdPZnZ3ej9avgcm9HJcRmdGj0uTflANTN3RsQ5wD9RORXymmYEe2FZk/ar4XNsRifHZXRq6Lg05chdktRa3kNVkkrIcJekEjLcJamEShfuEfG2iFgREVdHxF+2uh5VRMT7ImJ5RNzS6lr0RhFxavF6uT0iTmh1PaqIiJkR8eOIuCUivlrv9mMi3CPimojYHBFr92qfGxG/iogNEXFB0fwp4JbM/BLwyREvdhypZ1wy85nMPLs1lY4/dY7N/y5eLwuAv2hBueNGneOyPjO/Avw5UPcpkmMi3IFrgbn9G/pdnOwk4DDgrIg4jMoXpp4ruu0awRrHo2upfVw0sq6l/rH5drFezXMtdYxLRHwSeAi4t95fNCbCPTN/DrywV/Oei5Nl5v8Ddl+crI9KwMMY+fvGqjrHRSOonrGJiu8D/ycznxjpWseTel8zmbkyM/8zUPcU81gOv3b+cIQOlVBvB24DPh0RSxl9X70eD6qOS0RMiYgfA0dFxIWtKW3cG+g1cy5wPHB6RHylFYWNcwO9Zo6LiCsi4irg7np3OpbvxBRV2jIzfwd8YaSL0R4DjctWwOBorYHG5grgipEuRnsMNC4PAA8Mdadj+ci9URcnU2M5LqOXYzM6NWVcxnK4Pw7MiIjpEfEW4ExgZYtrkuMymjk2o1NTxmVMhHtE3AA8DPxJRPRFxNmZuRPYfXGy9cDNTbw4mapwXEYvx2Z0Gslx8cJhklRCY+LIXZJUH8NdkkrIcJekEjLcJamEDHdJKiHDXZJKyHCXpBIy3CWphAx3SSqh/w/Rax23ILGLtgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data Visualisation\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Plot a Log based Histogram - Length of the text\n",
    "\n",
    "plt.xscale('log')\n",
    "bins = 1.15**(np.arange(0,50))\n",
    "plt.hist(df[df['label']=='ham']['length'],bins=bins,alpha=0.8)\n",
    "plt.hist(df[df['label']=='spam']['length'],bins=bins,alpha=0.8)\n",
    "plt.legend(('ham','spam'))\n",
    "plt.show()\n",
    "\n",
    "# The Insights here is Spam Emails have Larger words/contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAARJElEQVR4nO3df5CdVX3H8fc3PyBghWgSGdxFNozRRnInomsSamgHw8REGsMAmUJFE82Q1iFRoEqg0xkc/adqxyjYSQnGGmcyGAtMQ4qlVVAqM4Bs+OEmpjYRUriGwhpDmgLRJJz+sc+GTdjN3s3eZ+/ek/drJrPPj/Oc/V7O3M8ezj732UgpIUnKy6hGFyBJqj/DXZIyZLhLUoYMd0nKkOEuSRky3CUpQ2MaXQDAxIkTU1tbW6PLkKSmsnnz5t+klCb1dW5EhHtbWxsdHR2NLkOSmkpE/Hd/51yWkaQMGe6SlCHDXZIyNCLW3CWpFgcOHKBarbJ///5GlzKsxo0bR2trK2PHjq35GsNdUtOoVqu8+c1vpq2tjYhodDnDIqXE7t27qVarTJ48uebrXJaR1DT279/PhAkTTphgB4gIJkyYMOj/WzHcJTWVEynYexzPazbcJalGO3fuZNq0aY0uoyauufdjwa0PldLvphWzS+lXOhHV+32a0/vTmbskDcKhQ4e4+uqrOffcc5k7dy6vvvoqt99+Ox/4wAeYPn06l112Ga+88goAS5Ys4dOf/jQXXngh55xzDg8++CCf+tSnmDp1KkuWLCm1TsNdkgZh+/btXHPNNWzdupXx48dz1113cemll/LYY4/x1FNPMXXqVNauXXu4/Z49e3jggQdYtWoVCxYs4LrrrmPr1q10dnby5JNPllan4S5JgzB58mTe+973AvD+97+fnTt3smXLFi644AIqlQrr169n69ath9svWLCAiKBSqXDGGWdQqVQYNWoU5557Ljt37iytTsNdkgbh5JNPPrw9evRoDh48yJIlS/jmN79JZ2cnN9988xG3Lfa0HzVq1BHXjho1ioMHD5ZWp+EuSUO0b98+zjzzTA4cOMD69esbXQ7g3TKSNGRf+tKXmDlzJmeffTaVSoV9+/Y1uiQipdToGmhvb08j7Xnu3gopjTzbtm1j6tSpjS6jIfp67RGxOaXU3ld7l2UkKUOGuyRlqOnX3MtaPpGkZubMXZIyZLhLUoYMd0nKkOEuSRlq+l+oSjqB3fYn9e3vLx6sb38N5Mxdkmr08ssvc/HFFzN9+nSmTZvGhg0baGtrY+XKlcyYMYMZM2awY8cOADZt2sTMmTM577zzuOiii3jhhRcA+MIXvsDixYuZO3cubW1t3H333dxwww1UKhXmzZvHgQMH6lKr4S5JNbrvvvt4+9vfzlNPPcWWLVuYN28eAKeddho/+9nPWL58Oddeey0As2fP5pFHHuGJJ57giiuu4Ctf+crhfn71q19x7733snHjRq666iouvPBCOjs7OeWUU7j33nvrUqvhLkk1qlQq/OhHP2LlypX89Kc/5fTTTwfgyiuvPPz14YcfBqBarfLhD3+YSqXCV7/61SMeAzx//nzGjh1LpVLh0KFDh39IVCqVuj0G2HCXpBq9613vYvPmzVQqFW666Sa++MUvAkf+Aeue7RUrVrB8+XI6Ozu57bbb+n0M8NixYw9fU8/HABvuklSjXbt2ceqpp3LVVVfxuc99jscffxyADRs2HP56/vnnA7B3715aWloAWLdu3bDX6t0yklSjzs5OPv/5zx+eca9evZrLL7+c3/3ud8ycOZPXXnuNO+64A+j+xemiRYtoaWlh1qxZPPPMM8Naa9M/8rfZni3jI3+l4zcSH/nb1tZGR0cHEydOLPX7+MhfSZLLMpI0FGX+keuhqGnmHhHXRcTWiNgSEXdExLiImBwRj0bE9ojYEBEnFW1PLvZ3FOfbynwBkqQ3GjDcI6IF+AzQnlKaBowGrgC+DKxKKU0B9gBLi0uWAntSSu8EVhXtJKkuRsLvCYfb8bzmWtfcxwCnRMQY4FTgeeBDwJ3F+XXAJcX2wmKf4vyc6H0TqCQdp3HjxrF79+4TKuBTSuzevZtx48YN6roB19xTSr+OiL8DngVeBf4d2Ay8lFLqudu+CrQU2y3Ac8W1ByNiLzAB+M2gKpOko7S2tlKtVunq6mp0KcNq3LhxtLa2DuqaAcM9It5C92x8MvAS8E/A/D6a9vwo7WuW/oYfsxGxDFgG8I53vKPGciWdyMaOHcvkyZMbXUZTqGVZ5iLgmZRSV0rpAHA38EfA+GKZBqAV2FVsV4GzAIrzpwO/PbrTlNKalFJ7Sql90qRJQ3wZkqTeagn3Z4FZEXFqsXY+B/gF8GPg8qLNYmBjsX1PsU9x/oF0Ii2QSdIIMGC4p5QepfsXo48DncU1a4CVwPURsYPuNfW1xSVrgQnF8euBG0uoW5J0DDV9iCmldDNw81GHnwZm9NF2P7Bo6KVJko6Xjx+QpAwZ7pKUIZ8tk4kyno7pEyyl5uXMXZIyZLhLUoYMd0nKkOEuSRky3CUpQ4a7JGXIcJekDBnukpQhw12SMuQnVIdZGZ8klaSjOXOXpAwZ7pKUIcNdkjJkuEtShgx3ScqQ4S5JGTLcJSlDhrskZchwl6QMGe6SlCHDXZIyZLhLUoYMd0nKkOEuSRky3CUpQ4a7JGXIcJekDBnukpQhw12SMmS4S1KGDHdJypDhLkkZMtwlKUM1hXtEjI+IOyPiPyNiW0ScHxFvjYgfRsT24utbirYREbdExI6I+HlEvK/clyBJOlqtM/dvAPellP4QmA5sA24E7k8pTQHuL/YB5gNTin/LgNV1rViSNKABwz0iTgP+GFgLkFL6fUrpJWAhsK5otg64pNheCHw3dXsEGB8RZ9a9cklSv2qZuZ8DdAH/GBFPRMS3IuJNwBkppecBiq9vK9q3AM/1ur5aHDtCRCyLiI6I6Ojq6hrSi5AkHamWcB8DvA9YnVI6D3iZ15dg+hJ9HEtvOJDSmpRSe0qpfdKkSTUVK0mqTS3hXgWqKaVHi/076Q77F3qWW4qvL/Zqf1av61uBXfUpV5JUiwHDPaX0P8BzEfHu4tAc4BfAPcDi4thiYGOxfQ/wieKumVnA3p7lG0nS8BhTY7sVwPqIOAl4Gvgk3T8Yvh8RS4FngUVF2x8AHwF2AK8UbSVJw6imcE8pPQm093FqTh9tE3DNEOuSJA2Bn1CVpAwZ7pKUIcNdkjJkuEtShgx3ScqQ4S5JGTLcJSlDhrskZchwl6QMGe6SlCHDXZIyZLhLUoYMd0nKkOEuSRky3CUpQ4a7JGXIcJekDBnukpQhw12SMmS4S1KGDHdJypDhLkkZMtwlKUNjGl2ARq4Ftz5USr+bVswupV9Jr3PmLkkZMtwlKUOGuyRlyHCXpAwZ7pKUIcNdkjJkuEtShgx3ScqQ4S5JGTLcJSlDhrskZajmcI+I0RHxRET8S7E/OSIejYjtEbEhIk4qjp9c7O8ozreVU7okqT+Dmbl/FtjWa//LwKqU0hRgD7C0OL4U2JNSeiewqmgnSRpGNYV7RLQCFwPfKvYD+BBwZ9FkHXBJsb2w2Kc4P6doL0kaJrXO3L8O3AC8VuxPAF5KKR0s9qtAS7HdAjwHUJzfW7SXJA2TAcM9Iv4UeDGltLn34T6aphrO9e53WUR0RERHV1dXTcVKkmpTy8z9g8BHI2In8D26l2O+DoyPiJ4/9tEK7Cq2q8BZAMX504HfHt1pSmlNSqk9pdQ+adKkIb0ISdKRBgz3lNJNKaXWlFIbcAXwQErpY8CPgcuLZouBjcX2PcU+xfkHUkpvmLlLksozlPvcVwLXR8QOutfU1xbH1wITiuPXAzcOrURJ0mAN6m+oppR+Avyk2H4amNFHm/3AojrUJkk6Tn5CVZIyZLhLUoYMd0nKkOEuSRky3CUpQ4a7JGVoULdCSvWw4NaHSul304rZpfQrNSNn7pKUIcNdkjJkuEtShgx3ScqQ4S5JGTLcJSlDhrskZchwl6QMGe6SlCHDXZIyZLhLUoYMd0nKkOEuSRky3CUpQ4a7JGXIcJekDBnukpQhw12SMmS4S1KGDHdJypDhLkkZMtwlKUOGuyRlyHCXpAwZ7pKUIcNdkjJkuEtShgx3ScqQ4S5JGRow3CPirIj4cURsi4itEfHZ4vhbI+KHEbG9+PqW4nhExC0RsSMifh4R7yv7RUiSjlTLzP0g8FcppanALOCaiHgPcCNwf0ppCnB/sQ8wH5hS/FsGrK571ZKkYxow3FNKz6eUHi+29wHbgBZgIbCuaLYOuKTYXgh8N3V7BBgfEWfWvXJJUr8GteYeEW3AecCjwBkppeeh+wcA8LaiWQvwXK/LqsWxo/taFhEdEdHR1dU1+MolSf2qOdwj4g+Au4BrU0r/e6ymfRxLbziQ0pqUUntKqX3SpEm1liFJqkFN4R4RY+kO9vUppbuLwy/0LLcUX18sjleBs3pd3grsqk+5kqRa1HK3TABrgW0ppa/1OnUPsLjYXgxs7HX8E8VdM7OAvT3LN5Kk4TGmhjYfBD4OdEbEk8Wxvwb+Fvh+RCwFngUWFed+AHwE2AG8AnyyrhVLkgY0YLinlB6i73V0gDl9tE/ANUOsS5I0BLXM3KWmsODWh+re56YVs+vepzQcfPyAJGXIcJekDLkso3597aXPltLv9eO/UUq/kl5nuGvY+UNDKp/LMpKUIcNdkjLkskwmylrqkNScnLlLUoYMd0nKkOEuSRky3CUpQ4a7JGXIu2WGmXe1SBoOztwlKUOGuyRlyHCXpAwZ7pKUIcNdkjJkuEtShgx3ScqQ4S5JGTLcJSlDfkK1H36SVFIzc+YuSRky3CUpQ4a7JGXIcJekDPkLVekYFtz6UCn9bloxu5R+pR7O3CUpQ87clY0ybl+9fvw36t6nNBycuUtShgx3ScpQ0y/L+ElSSXojZ+6SlKGmn7lLzchbLFW2UmbuETEvIn4ZETsi4sYyvockqX91D/eIGA38PTAfeA9wZUS8p97fR5LUvzKWZWYAO1JKTwNExPeAhcAvSvheUqnK+oV9WffPu9yjHmWEewvwXK/9KjDz6EYRsQxYVuz+X0T8stg+HdjbT999nZsI/Oa4qy3HsV5DI/sd7PW1tB9qm/7O9Xc8k/G+oKR+j/vaY7aPz9Tc7/GMdX/nMhnrUvs9u98zKaW6/gMWAd/qtf9x4NZBXL9mMOeAjnq/hjr8N+j3NTSy38FeX0v7obbp79wxjjveDRrrWtodz1j3d86xHtq/Mn6hWgXO6rXfCuwaxPWbjvPcSFJWnUPtd7DX19J+qG36O9csYw0jc7zLGOta2h3v+7dZxnskjnWfovipUb8OI8YA/wXMAX4NPAb8eUppa12/0evfryOl1F5G3xp5HO8Th2M9NHVfc08pHYyI5cC/AaOBb5cV7IU1JfatkcfxPnE41kNQ95m7JKnxfPyAJGXIcJekDBnukpSh7MI9It4UEesi4vaI+Fij61F5IuKciFgbEXc2uhaVLyIuKd7XGyNibqPrGemaItwj4tsR8WJEbDnqeF8PKLsUuDOldDXw0WEvVkMymLFOKT2dUlramEpVD4Mc738u3tdLgD9rQLlNpSnCHfgOMK/3gWM8oKyV1x9/cGgYa1R9fIfax1rN7zsMfrz/pjivY2iKcE8p/Qfw26MOH35AWUrp90DPA8qqdAc8NMnr0+sGOdZqcoMZ7+j2ZeBfU0qPD3etzaaZw6+vB5S1AHcDl0XEaprnI806tj7HOiImRMQ/AOdFxE2NKU0l6O+9vQK4CLg8Iv6yEYU1k2b+S0zRx7GUUnoZ+ORwF6NS9TfWuwHf5Pnpb7xvAW4Z7mKaVTPP3If6gDI1D8f6xOJ410Ezh/tjwJSImBwRJwFXAPc0uCaVw7E+sTjeddAU4R4RdwAPA++OiGpELE0pHQR6HlC2Dfh+yQ8o0zBwrE8sjnd5fHCYJGWoKWbukqTBMdwlKUOGuyRlyHCXpAwZ7pKUIcNdkjJkuEtShgx3ScqQ4S5JGfp/0DKY4fCtzG0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Punctuations in the Text\n",
    "\n",
    "plt.xscale('log')\n",
    "bins = 1.5**(np.arange(0,15))\n",
    "plt.hist(df[df['label']=='ham']['punct'],bins=bins,alpha=0.8)\n",
    "plt.hist(df[df['label']=='spam']['punct'],bins=bins,alpha=0.8)\n",
    "plt.legend(('ham','spam'))\n",
    "plt.show()\n",
    "\n",
    "# There is no Significant insights here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creation of Machine Learning Model - Plain without Count Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Feature and Label sets\n",
    "X = df[['length','punct']]  # note the double set of brackets\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Shape: (3733, 2)\n",
      "Testing Data Shape:  (1839, 2)\n"
     ]
    }
   ],
   "source": [
    "# Creation of Train and Test Data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "print('Training Data Shape:', X_train.shape)\n",
    "print('Testing Data Shape: ', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1547   46]\n",
      " [ 241    5]]\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Classifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_model = LogisticRegression(solver='lbfgs')\n",
    "\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Model Predictions\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "# Create a prediction set:\n",
    "predictions = lr_model.predict(X_test)\n",
    "\n",
    "# Print a confusion matrix\n",
    "print(metrics.confusion_matrix(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>1547</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>241</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ham  spam\n",
       "ham   1547    46\n",
       "spam   241     5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get Confusion Matrix as a Dataframe\n",
    "# You can make the confusion matrix less confusing by adding labels:\n",
    "df = pd.DataFrame(metrics.confusion_matrix(y_test,predictions), index=['ham','spam'], columns=['ham','spam'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.843936922240348\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.87      0.97      0.92      1593\n",
      "        spam       0.10      0.02      0.03       246\n",
      "\n",
      "    accuracy                           0.84      1839\n",
      "   macro avg       0.48      0.50      0.47      1839\n",
      "weighted avg       0.76      0.84      0.80      1839\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the overall accuracy\n",
    "print(metrics.accuracy_score(y_test,predictions))\n",
    "\n",
    "# Print a classification report\n",
    "print(metrics.classification_report(y_test,predictions))\n",
    "\n",
    "# As you can see the Accuracy was good, but it failed to predict Spam Miserably, see Precision/Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1583   10]\n",
      " [ 246    0]]\n",
      "0.8607939097335509\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.87      0.99      0.93      1593\n",
      "        spam       0.00      0.00      0.00       246\n",
      "\n",
      "    accuracy                           0.86      1839\n",
      "   macro avg       0.43      0.50      0.46      1839\n",
      "weighted avg       0.75      0.86      0.80      1839\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes Classifier\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb_model = MultinomialNB()\n",
    "\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "predictions = nb_model.predict(X_test)\n",
    "\n",
    "print(metrics.confusion_matrix(y_test,predictions))\n",
    "\n",
    "# Printing the Metrics\n",
    "\n",
    "print(metrics.accuracy_score(y_test,predictions))\n",
    "\n",
    "print(metrics.classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1515   78]\n",
      " [ 131  115]]\n",
      "0.8863512778684067\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.92      0.95      0.94      1593\n",
      "        spam       0.60      0.47      0.52       246\n",
      "\n",
      "    accuracy                           0.89      1839\n",
      "   macro avg       0.76      0.71      0.73      1839\n",
      "weighted avg       0.88      0.89      0.88      1839\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Classifier\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "svc_model = SVC(gamma='auto')\n",
    "svc_model.fit(X_train,y_train)\n",
    "\n",
    "predictions = svc_model.predict(X_test)\n",
    "print(metrics.confusion_matrix(y_test,predictions))\n",
    "\n",
    "# Printing the Metrics\n",
    "\n",
    "print(metrics.accuracy_score(y_test,predictions))\n",
    "\n",
    "print(metrics.classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction from Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Build a vocabulary\n",
    "\n",
    "The goal here is to build a numerical array from all the words that appear in every document. Later we'll create instances (vectors) for each individual document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this': 1, 'is': 2, 'a': 3, 'story': 4, 'about': 5, 'cats': 6, 'our': 7, 'feline': 8, 'pets': 9, 'are': 10, 'furry': 11, 'animals': 12}\n"
     ]
    }
   ],
   "source": [
    "vocab = {}\n",
    "i = 1\n",
    "\n",
    "with open('1.txt') as f:\n",
    "    x = f.read().lower().split()\n",
    "\n",
    "for word in x:\n",
    "    if word in vocab:\n",
    "        continue\n",
    "    else:\n",
    "        vocab[word]=i\n",
    "        i+=1\n",
    "\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this': 1, 'is': 2, 'a': 3, 'story': 4, 'about': 5, 'cats': 6, 'our': 7, 'feline': 8, 'pets': 9, 'are': 10, 'furry': 11, 'animals': 12, 'surfing': 13, 'catching': 14, 'waves': 15, 'fun': 16, 'popular': 17, 'water': 18, 'sport': 19}\n"
     ]
    }
   ],
   "source": [
    "with open('2.txt') as f:\n",
    "    x = f.read().lower().split()\n",
    "\n",
    "for word in x:\n",
    "    if word in vocab:\n",
    "        continue\n",
    "    else:\n",
    "        vocab[word]=i\n",
    "        i+=1\n",
    "\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Showing How Count Vectorizer Works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1.txt', 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an empty vector with space for each word in the vocabulary:\n",
    "one = ['1.txt']+[0]*len(vocab)\n",
    "one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1.txt', 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map the frequencies of each word in 1.txt to our vector:\n",
    "with open('1.txt') as f:\n",
    "    x = f.read().lower().split()\n",
    "    \n",
    "for word in x:\n",
    "    one[vocab[word]]+=1\n",
    "    \n",
    "one\n",
    "# We can see that most of the words in 1.txt appear only once, although \"cats\" appears twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same for the second document:\n",
    "two = ['2.txt']+[0]*len(vocab)\n",
    "\n",
    "with open('2.txt') as f:\n",
    "    x = f.read().lower().split()\n",
    "    \n",
    "for word in x:\n",
    "    two[vocab[word]]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1.txt', 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "['2.txt', 1, 3, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Compare the two vectors:\n",
    "print(f'{one}\\n{two}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By comparing the vectors we see that some words are common to both, some appear only in 1.txt, others only in 2.txt. Extending this logic to tens of thousands of documents, we would see the vocabulary dictionary grow to hundreds of thousands of words. Vectors would contain mostly zero values, making them sparse matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Bag of Words and Tf-idf\n",
    "\n",
    "In the above examples, each vector can be considered a bag of words. By itself these may not be helpful until we consider term frequencies, or how often individual words appear in documents. A simple way to calculate term frequencies is to divide the number of occurrences of a word by the total number of words in the document. In this way, the number of times a word appears in large documents can be compared to that of smaller documents.\n",
    "\n",
    "However, it may be hard to differentiate documents based on term frequency if a word shows up in a majority of documents. To handle this we also consider inverse document frequency, which is the total number of documents divided by the number of documents that contain the word. In practice we convert this value to a logarithmic scale, as described here.\n",
    "Together these terms become tf-idf.\n",
    "\n",
    "##### Stop Words and Word Stems\n",
    "\n",
    "Some words like \"the\" and \"and\" appear so frequently, and in so many documents, that we needn't bother counting them. Also, it may make sense to only record the root of a word, say cat in place of both cat and cats. This will shrink our vocab array and improve performance.\n",
    "\n",
    "### Tokenization and Tagging\n",
    "\n",
    "When we created our vectors the first thing we did was split the incoming text on whitespace with .split(). This was a crude form of tokenization - that is, dividing a document into individual words. In this simple example we didn't worry about punctuation or different parts of speech. In the real world we rely on some fairly sophisticated morphology to parse text appropriately.\n",
    "\n",
    "Once the text is divided, we can go back and tag our tokens with information about parts of speech, grammatical dependencies, etc. This adds more dimensions to our data and enables a deeper understanding of the context of specific documents. For this reason, vectors become high dimensional sparse matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction from Text\n",
    "\n",
    "In the above,  we applied a simple SVC classification model to the SMSSpamCollection dataset. We tried to predict the ham/spam label based on message length and punctuation counts. In this section we'll actually look at the text of each message and try to perform a classification based on content. We'll take advantage of some of scikit-learn's feature extraction tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We Use the Same Dataset as Above\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('smsspamcollection.tsv', sep='\\t')\n",
    "\n",
    "X = df['message']  # this time we want to look at the text\n",
    "y = df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scikit-learn's CountVectorizer\n",
    "\n",
    "Text preprocessing, tokenizing and the ability to filter out stopwords are all included in CountVectorizer, which builds a dictionary of features and transforms documents to feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3733, 7082)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3733 - No of Rows, 7082 - Feature Names/Words. \n",
    "\n",
    "This shows that our training set is comprised of 3733 documents, and 7082 features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform Counts to Frequencies with Tf-idf\n",
    "\n",
    "While counting words is helpful, longer documents will have higher average count values than shorter documents, even though they might talk about the same topics.\n",
    "\n",
    "To avoid this we can simply divide the number of occurrences of each word in a document by the total number of words in the document: these new features are called tf for Term Frequencies.\n",
    "\n",
    "Another refinement on top of tf is to downscale weights for words that occur in many documents in the corpus and are therefore less informative than those that occur only in a smaller portion of the corpus.\n",
    "\n",
    "This downscaling is called tf–idf for “Term Frequency times Inverse Document Frequency”.\n",
    "Both tf and tf–idf can be computed as follows using TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3733, 7082)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the fit_transform() method actually performs two operations: it fits an estimator to the data and then transforms our count-matrix to a tf-idf representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3733, 7082)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can Combine the above 2 Steps (CV + TFIDF Transformer) with TfidVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train) # remember to use the original X_train set\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creation of Machine Learning Model - with Text Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Support Vector Classifier\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "clf = LinearSVC()\n",
    "clf.fit(X_train_tfidf,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build a Pipeline\n",
    "\n",
    "Remember that only our training set has been vectorized into a full vocabulary. In order to perform an analysis on our test set we'll have to submit it to the same procedures. Fortunately scikit-learn offers a Pipeline class that behaves like a compound classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1586    7]\n",
      " [  12  234]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      1.00      0.99      1593\n",
      "        spam       0.97      0.95      0.96       246\n",
      "\n",
      "    accuracy                           0.99      1839\n",
      "   macro avg       0.98      0.97      0.98      1839\n",
      "weighted avg       0.99      0.99      0.99      1839\n",
      "\n",
      "0.989668297988037\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.svm import LinearSVC\n",
    "\n",
    "text_clf = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                     ('clf', LinearSVC()),\n",
    "])\n",
    "\n",
    "# Feed the training data through the pipeline\n",
    "text_clf.fit(X_train, y_train)  \n",
    "\n",
    "# Get the Predictions\n",
    "# Form a prediction set\n",
    "predictions = text_clf.predict(X_test)\n",
    "\n",
    "# Print out the Metrics\n",
    "\n",
    "# Report the confusion matrix\n",
    "from sklearn import metrics\n",
    "print(metrics.confusion_matrix(y_test,predictions))\n",
    "\n",
    "# Print a classification report\n",
    "print(metrics.classification_report(y_test,predictions))\n",
    "\n",
    "# Print the overall accuracy\n",
    "print(metrics.accuracy_score(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the text of the messages, our model performed exceedingly well; it correctly predicted spam 98.97% of the time!\n",
    "Now let's apply what we've learned to a text classification project involving positive and negative movie reviews."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
