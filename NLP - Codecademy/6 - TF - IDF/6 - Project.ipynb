{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the News Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Newspapers and their online formats supply the public with the information we need to understand the events occurring in the world around us. From politics to sports, the news keeps us informed, in the loop, and ready to make decisions about how to act in a rapidly changing world.\n",
    "\n",
    "Given the vast amount of news articles in circulation, identifying and organizing articles by topic is a useful activity. This can help you sift through the enormous amount of information out there so you can find the news relevant to your interests, or even allow you to build a news recommendation engine!\n",
    "\n",
    "[The News International](https://www.thenews.com.pk/) is the largest English language newspaper in Pakistan, covering local and international news across a variety of sectors. A selection of articles from a [Kaggle Dataset of The News International articles](https://www.kaggle.com/asad1m9a9h6mood/news-articles) is taken.\n",
    "\n",
    "In this project you will use term frequency-inverse document frequency (tf-idf) to analyze each article’s content and uncover the terms that best describe each article, providing quick insight into each article’s topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Preprocessing Function\n",
    "\n",
    "import nltk, re\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "normalizer = WordNetLemmatizer()\n",
    "\n",
    "def get_part_of_speech(word):\n",
    "    probable_part_of_speech = wordnet.synsets(word)\n",
    "    pos_counts = Counter()\n",
    "    pos_counts[\"n\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"n\"]  )\n",
    "    pos_counts[\"v\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"v\"]  )\n",
    "    pos_counts[\"a\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"a\"]  )\n",
    "    pos_counts[\"r\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"r\"]  )\n",
    "    most_likely_part_of_speech = pos_counts.most_common(1)[0][0]\n",
    "    return most_likely_part_of_speech\n",
    "\n",
    "def preprocess_text(text):\n",
    "    cleaned = re.sub(r'\\W+', ' ', text).lower()\n",
    "    tokenized = word_tokenize(cleaned)\n",
    "    normalized = \" \".join([normalizer.lemmatize(token, get_part_of_speech(token)) for token in tokenized if not re.match(r'\\d+',token)])\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HONG KONG:  Hong Kong shares opened 0.66 percent lower Monday following a tepid lead from Wall Street, as the first full week of the new year kicked off. The benchmark Hang Seng Index dipped 158.63 points to 23,699.19.\n",
      "  Are the tf-idf scores the same?\n",
      "0       No, something is wrong :(\n",
      "        Article 1  Article 2  Article 3  Article 4  Article 5  Article 6  \\\n",
      "abbasi          0          0          0          1          0          0   \n",
      "abide           1          0          0          0          0          0   \n",
      "about           0          0          0          0          0          0   \n",
      "accord          0          0          1          0          0          0   \n",
      "add             1          0          0          0          0          0   \n",
      "...           ...        ...        ...        ...        ...        ...   \n",
      "world           0          0          0          0          0          3   \n",
      "would           0          0          0          1          0          0   \n",
      "year            0          1          0          0          0          0   \n",
      "yi              0          0          0          0          0          0   \n",
      "yuan            0          0          0          0          0          0   \n",
      "\n",
      "        Article 7  Article 8  Article 9  Article 10  \n",
      "abbasi          0          0          0           0  \n",
      "abide           0          0          0           0  \n",
      "about           1          0          0           0  \n",
      "accord          0          0          0           0  \n",
      "add             0          0          1           0  \n",
      "...           ...        ...        ...         ...  \n",
      "world           0          0          0           0  \n",
      "would           0          0          1           0  \n",
      "year            0          0          0           0  \n",
      "yi              0          0          0           2  \n",
      "yuan            0          0          0           2  \n",
      "\n",
      "[353 rows x 10 columns]\n",
      "        Article 1  Article 2  Article 3  Article 4  Article 5  Article 6  \\\n",
      "abbasi   0.000000   0.000000   0.000000   2.704748        0.0   0.000000   \n",
      "abide    2.704748   0.000000   0.000000   0.000000        0.0   0.000000   \n",
      "about    0.000000   0.000000   0.000000   0.000000        0.0   0.000000   \n",
      "accord   0.000000   0.000000   2.704748   0.000000        0.0   0.000000   \n",
      "add      2.299283   0.000000   0.000000   0.000000        0.0   0.000000   \n",
      "...           ...        ...        ...        ...        ...        ...   \n",
      "world    0.000000   0.000000   0.000000   0.000000        0.0   8.114244   \n",
      "would    0.000000   0.000000   0.000000   2.299283        0.0   0.000000   \n",
      "year     0.000000   2.704748   0.000000   0.000000        0.0   0.000000   \n",
      "yi       0.000000   0.000000   0.000000   0.000000        0.0   0.000000   \n",
      "yuan     0.000000   0.000000   0.000000   0.000000        0.0   0.000000   \n",
      "\n",
      "        Article 7  Article 8  Article 9  Article 10  \n",
      "abbasi   0.000000        0.0   0.000000    0.000000  \n",
      "abide    0.000000        0.0   0.000000    0.000000  \n",
      "about    2.704748        0.0   0.000000    0.000000  \n",
      "accord   0.000000        0.0   0.000000    0.000000  \n",
      "add      0.000000        0.0   2.299283    0.000000  \n",
      "...           ...        ...        ...         ...  \n",
      "world    0.000000        0.0   0.000000    0.000000  \n",
      "would    0.000000        0.0   2.299283    0.000000  \n",
      "year     0.000000        0.0   0.000000    0.000000  \n",
      "yi       0.000000        0.0   0.000000    5.409496  \n",
      "yuan     0.000000        0.0   0.000000    5.409496  \n",
      "\n",
      "[353 rows x 10 columns]\n",
      "        Article 1  Article 2  Article 3  Article 4  Article 5  Article 6  \\\n",
      "abbasi   0.000000   0.000000   0.000000   0.101646        0.0   0.000000   \n",
      "abide    0.088177   0.000000   0.000000   0.000000        0.0   0.000000   \n",
      "about    0.000000   0.000000   0.000000   0.000000        0.0   0.000000   \n",
      "accord   0.000000   0.000000   0.078884   0.000000        0.0   0.000000   \n",
      "add      0.074958   0.000000   0.000000   0.000000        0.0   0.000000   \n",
      "...           ...        ...        ...        ...        ...        ...   \n",
      "world    0.000000   0.000000   0.000000   0.000000        0.0   0.292179   \n",
      "would    0.000000   0.000000   0.000000   0.086408        0.0   0.000000   \n",
      "year     0.000000   0.176771   0.000000   0.000000        0.0   0.000000   \n",
      "yi       0.000000   0.000000   0.000000   0.000000        0.0   0.000000   \n",
      "yuan     0.000000   0.000000   0.000000   0.000000        0.0   0.000000   \n",
      "\n",
      "        Article 7  Article 8  Article 9  Article 10  \n",
      "abbasi   0.000000        0.0   0.000000    0.000000  \n",
      "abide    0.000000        0.0   0.000000    0.000000  \n",
      "about    0.111454        0.0   0.000000    0.000000  \n",
      "accord   0.000000        0.0   0.000000    0.000000  \n",
      "add      0.000000        0.0   0.097672    0.000000  \n",
      "...           ...        ...        ...         ...  \n",
      "world    0.000000        0.0   0.000000    0.000000  \n",
      "would    0.000000        0.0   0.097672    0.000000  \n",
      "year     0.000000        0.0   0.000000    0.000000  \n",
      "yi       0.000000        0.0   0.000000    0.302881  \n",
      "yuan     0.000000        0.0   0.000000    0.302881  \n",
      "\n",
      "[353 rows x 10 columns]\n",
      "Article 1    fare\n",
      "dtype: object\n",
      "Article 2    hong\n",
      "dtype: object\n",
      "Article 3    sugar\n",
      "dtype: object\n",
      "Article 4    petrol\n",
      "dtype: object\n",
      "Article 5    engine\n",
      "dtype: object\n",
      "Article 6    australia\n",
      "dtype: object\n",
      "Article 7    car\n",
      "dtype: object\n",
      "Article 8    railway\n",
      "dtype: object\n",
      "Article 9    cabinet\n",
      "dtype: object\n",
      "Article 10    china\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from articles import articles\n",
    "\n",
    "# import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "\n",
    "# view any article\n",
    "print(articles[1])\n",
    "\n",
    "# preprocess articles\n",
    "processed_articles = [preprocess_text(article) for article in articles]\n",
    "\n",
    "# initialize and fit CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "counts = vectorizer.fit_transform(processed_articles)\n",
    "\n",
    "# convert counts to tf-idf - Fit Transform Gives Count\n",
    "transformer = TfidfTransformer(norm=None)\n",
    "tfidf_scores_transformed = transformer.fit_transform(counts)\n",
    "\n",
    "# initialize and fit TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_scores = vectorizer.fit_transform(processed_articles)\n",
    "\n",
    "# check if tf-idf scores are equal\n",
    "if np.allclose(tfidf_scores_transformed.todense(), tfidf_scores.todense()):\n",
    "    print(pd.DataFrame({'Are the tf-idf scores the same?':['YES']}))\n",
    "else:\n",
    "    print(pd.DataFrame({'Are the tf-idf scores the same?':['No, something is wrong :(']}))\n",
    "\n",
    "# get vocabulary of terms\n",
    "try:\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# get article index\n",
    "try:\n",
    "    article_index = [f\"Article {i+1}\" for i in range(len(articles))]\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# create pandas DataFrame with word counts\n",
    "try:\n",
    "    df_word_counts = pd.DataFrame(counts.T.todense(), index=feature_names, columns=article_index)\n",
    "    print(df_word_counts)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# create pandas DataFrame(s) with tf-idf scores\n",
    "try:\n",
    "    df_tf_idf = pd.DataFrame(tfidf_scores_transformed.T.todense(), index=feature_names, columns=article_index)\n",
    "    print(df_tf_idf)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    df_tf_idf = pd.DataFrame(tfidf_scores.T.todense(), index=feature_names, columns=article_index)\n",
    "    print(df_tf_idf)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# get highest scoring tf-idf term for each article\n",
    "for i in range(1, len(articles) + 1):\n",
    "    print(df_tf_idf[[f'Article {i}']].idxmax())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
