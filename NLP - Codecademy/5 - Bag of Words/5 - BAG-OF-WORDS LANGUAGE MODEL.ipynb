{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Bag-of-Words\n",
    "\n",
    "“A bag-of-words is all you need,” some NLPers have decreed.\n",
    "\n",
    "The bag-of-words language model is a simple-yet-powerful tool to have up your sleeve when working on natural language processing (NLP). The model has many, many use cases including:\n",
    "\n",
    "- determining topics in a song\n",
    "- filtering spam from your inbox\n",
    "- finding out if a tweet has positive or negative sentiment\n",
    "- creating word clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'preprocessing'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-20fd9eb1e85a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#from spam_data import training_spam_docs, training_doc_tokens, training_labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnaive_bayes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpreprocess_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Add your email text to test_text between the triple quotes:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'preprocessing'"
     ]
    }
   ],
   "source": [
    "# Sample Code from CodeAcademy\n",
    "\n",
    "from spam_data import training_spam_docs, training_doc_tokens, training_labels\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from preprocessing import preprocess_text\n",
    "\n",
    "# Add your email text to test_text between the triple quotes:\n",
    "test_text = \"\"\"\n",
    "I agree that the past few weeks have been difficult, but organizations are steadily evolving to adapt to a new ‘normal’ like never before.\n",
    "\n",
    "To support your transition and enable you to overcome the challenges of continuity in hiring, Learning & Development (L&D) and employee engagement initiatives, Mercer | Mettl is offering a suite of premium services free of charge!\n",
    "\"\"\"\n",
    "\n",
    "test_tokens = preprocess_text(test_text)\n",
    "\n",
    "def create_features_dictionary(document_tokens):\n",
    "    features_dictionary = {}\n",
    "    index = 0\n",
    "    for token in document_tokens:\n",
    "        if token not in features_dictionary:\n",
    "            features_dictionary[token] = index\n",
    "            index += 1\n",
    "    return features_dictionary\n",
    "\n",
    "def tokens_to_bow_vector(document_tokens, features_dictionary):\n",
    "    bow_vector = [0] * len(features_dictionary)\n",
    "    for token in document_tokens:\n",
    "        if token in features_dictionary:\n",
    "            feature_index = features_dictionary[token]\n",
    "            bow_vector[feature_index] += 1\n",
    "    return bow_vector\n",
    "\n",
    "bow_sms_dictionary = create_features_dictionary(training_doc_tokens)\n",
    "training_vectors = [tokens_to_bow_vector(training_doc, bow_sms_dictionary) for training_doc in training_spam_docs]\n",
    "test_vectors = [tokens_to_bow_vector(test_tokens, bow_sms_dictionary)]\n",
    "\n",
    "spam_classifier = MultinomialNB()\n",
    "spam_classifier.fit(training_vectors, training_labels)\n",
    "\n",
    "predictions = spam_classifier.predict(test_vectors)\n",
    "\n",
    "print(\"Looks like a normal email!\" if predictions[0] == 0 else \"You've got spam!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-What?\n",
    "\n",
    "Bag-of-words (BoW) is a statistical language model based on word count. Say what?\n",
    "\n",
    "Let’s start with that first part: a statistical language model is a way for computers to make sense of language based on probability. For example, let’s say we have the text:\n",
    "\n",
    "“Five fantastic fish flew off to find faraway functions. Maybe find another five fantastic fish?”\n",
    "\n",
    "A statistical language model focused on the starting letter for words might take this text and predict that words are most likely to start with the letter “f” because 11 out of 15 words begin that way. A different statistical model that pays attention to word order might tell us that the word “fish” tends to follow the word “fantastic.”\n",
    "\n",
    "Bag-of-words does not give a flying fish about word starts or word order though; its sole concern is word count — how many times each word appears in a document.\n",
    "\n",
    "If you’re already familiar with statistical language models, you may also have heard BoW referred to as the unigram model. It’s technically a special case of another statistical model, the n-gram model, with n (the number of words in a sequence) set to 1.\n",
    "\n",
    "If you have no idea what n-grams are, don’t worry — we’ll dive deeper into them in another lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - BoW Dictionaries\n",
    "\n",
    "One of the most common ways to implement the BoW model in Python is as a dictionary with each key set to a word and each value set to the number of times that word appears. \n",
    "\n",
    "The words from the sentence go into the bag-of-words and come out as a dictionary of words with their corresponding counts. For statistical models, we call the text that we use to build the model our training data. Usually, we need to prepare our text data by breaking it up into documents (shorter strings of text, generally sentences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Preprocessing\n",
    "\n",
    "import nltk, re\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "normalizer = WordNetLemmatizer()\n",
    "\n",
    "def get_part_of_speech(word):\n",
    "    probable_part_of_speech = wordnet.synsets(word)\n",
    "    pos_counts = Counter()\n",
    "    pos_counts[\"n\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"n\"]  )\n",
    "    pos_counts[\"v\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"v\"]  )\n",
    "    pos_counts[\"a\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"a\"]  )\n",
    "    pos_counts[\"r\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"r\"]  )\n",
    "    most_likely_part_of_speech = pos_counts.most_common(1)[0][0]\n",
    "    return most_likely_part_of_speech\n",
    "\n",
    "def preprocess_text(text):\n",
    "    cleaned = re.sub(r'\\W+', ' ', text).lower()\n",
    "    tokenized = word_tokenize(cleaned)\n",
    "    normalized = [normalizer.lemmatize(token, get_part_of_speech(token)) for token in tokenized]\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i': 2, 'love': 1, 'fantastic': 2, 'fly': 2, 'fish': 3, 'these': 1, 'be': 1, 'just': 1, 'ok': 1, 'so': 1, 'maybe': 1, 'will': 1, 'find': 1, 'another': 1, 'few': 1}\n"
     ]
    }
   ],
   "source": [
    "# Create a BOW Model\n",
    "# Text preprocessing allows us to count words like “game” and “Games” as the same word token.\n",
    "\n",
    "# Define text_to_bow() below:\n",
    "def text_to_bow(some_text):\n",
    "    bow_dictionary = {}\n",
    "    tokens = preprocess_text(some_text)\n",
    "    for token in tokens:\n",
    "        if token in bow_dictionary:\n",
    "            bow_dictionary[token] += 1\n",
    "        else:\n",
    "            bow_dictionary[token] = 1\n",
    "    return bow_dictionary\n",
    "\n",
    "print(text_to_bow(\"I love fantastic flying fish. These flying fish are just ok, so maybe I will find another few fantastic fish...\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Introducing BoW Vectors\n",
    "\n",
    "Sometimes a dictionary just won’t fit the bill. Topic modelling applications, for example, require an implementation of bag-of-words that is a bit more mathematical: feature vectors.\n",
    "\n",
    "A feature vector is a numeric representation of an item’s important features. Each feature has its own column. If the feature exists for the item, you could represent that with a 1. If the feature does not exist for that item, you could represent that with a 0. A few monsters could be represented as vectors like so:\n",
    "\n",
    "            has_fangs\tmelts_in_water\thates_sunlight\thas_fur\n",
    "        vampire\t1\t           0\t           1\t      0\n",
    "    werewolf\t1\t           0\t           0\t      1\n",
    "        witch\t0\t           1\t           0       \t  0\n",
    "\n",
    "For bag-of-words, instead of monsters you would have documents and the features would be different words. And we don’t just care if a word is present in a document; we want to know how many times it occurred! Turning text into a BoW vector is known as feature extraction or vectorization.\n",
    "\n",
    "But how do we know which vector index corresponds to which word? When building BoW vectors, we generally create a features dictionary of all vocabulary in our training data (usually several documents) mapped to indices.\n",
    "\n",
    "For example, with “Five fantastic fish flew off to find faraway functions. Maybe find another five fantastic fish?” our dictionary might be:\n",
    "\n",
    "{'five': 0,\n",
    "'fantastic': 1,\n",
    "'fish': 2,\n",
    "'fly': 3,\n",
    "'off': 4,\n",
    "'to': 5,\n",
    "'find': 6,\n",
    "'faraway': 7,\n",
    "'function': 8,\n",
    "'maybe': 9,\n",
    "'another': 10}\n",
    "\n",
    "Using this dictionary, we can convert new documents into vectors using a vectorization function. For example, we can take a brand new sentence “Another five fish find another faraway fish.” — test data — and convert it to a vector that looks like:\n",
    "\n",
    "[1, 0, 2, 0, 0, 0, 1, 1, 0, 0, 2]\n",
    "\n",
    "The word ‘another’ appeared twice in the test data. If we look at the feature dictionary for ‘another’, we find that its index is 10. So when we go back and look at our vector, we’d expect the number at index 10 to be 2.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Building a Features Dictionary\n",
    "\n",
    "Now that you know what a bag-of-words vector looks like, you can create a function that builds them!\n",
    "\n",
    "First, we need a way of generating a features dictionary from a list of training documents. We can build a Python function to do that for us…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'five': 0, 'fantastic': 1, 'fish': 2, 'fly': 3, 'off': 4, 'to': 5, 'find': 6, 'faraway': 7, 'function': 8, 'maybe': 9, 'another': 10, 'my': 11, 'with': 12, 'a': 13, 'please': 14}\n"
     ]
    }
   ],
   "source": [
    "# Define create_features_dictionary() below:\n",
    "\n",
    "def create_features_dictionary(documents):\n",
    "    features_dictionary = {}\n",
    "    merged = \" \".join(documents)\n",
    "    tokens = preprocess_text(merged)\n",
    "    index = 0\n",
    "    for token in tokens:\n",
    "        if token not in features_dictionary:\n",
    "            features_dictionary[token] = index\n",
    "            index += 1\n",
    "    return features_dictionary, tokens\n",
    "\n",
    "training_documents = [\"Five fantastic fish flew off to find faraway functions.\", \"Maybe find another five fantastic fish?\", \"Find my fish with a function please!\"]\n",
    "\n",
    "print(create_features_dictionary(training_documents)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 - Building a BoW Vector\n",
    "\n",
    "Nice work! Time to put that dictionary of vocabulary to good use and build a bag-of-words vector from a new document.\n",
    "\n",
    "In Python, we can use a list to represent a vector. Each index in the list will correspond to a word and be set to its count.\n",
    "\n",
    "![BOW](https://s3.amazonaws.com/codecademy-content/courses/NLP/Building_vector.gif \"BOW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 2, 0, 0, 0, 1, 1, 0, 0, 2, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Building BOW Vector\n",
    "\n",
    "# Define text_to_bow_vector() below:\n",
    "def text_to_bow_vector(some_text, features_dictionary):\n",
    "    # Create list of 0s the length of features_dictionary and assign it \n",
    "    bow_vector = [0] * len(features_dictionary)\n",
    "    tokens = preprocess_text(some_text)\n",
    "    for token in tokens:\n",
    "        feature_index = features_dictionary[token]\n",
    "        bow_vector[feature_index] += 1\n",
    "    return bow_vector, tokens\n",
    "\n",
    "features_dictionary = {'function': 8, 'please': 14, 'find': 6, 'five': 0, 'with': 12, 'fantastic': 1, 'my': 11, 'another': 10, 'a': 13, 'maybe': 9, 'to': 5, 'off': 4, 'faraway': 7, 'fish': 2, 'fly': 3}\n",
    "\n",
    "text = \"Another five fish find another faraway fish.\"\n",
    "print(text_to_bow_vector(text, features_dictionary)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7 - It's All in the Bag\n",
    "\n",
    "It’s time to put create_features_dictionary() and tokens_to_bow_vector() together and use them in a spam filter we created that uses a Naive Bayes classifier. We’ve slightly modified the two functions for this use case, but they should still look familiar.\n",
    "\n",
    "Let’s see create_features_dictionary() and tokens_to_bow_vector() in action with real test data, helping fend off spam!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We Need a Spam data Pickle File here\n",
    "\n",
    "import pickle\n",
    "\n",
    "training_spam_docs, training_doc_tokens, training_labels, test_labels, test_spam_docs, raw_test_clean, raw_training_clean = pickle.load(open(\"spam_data.p\", \"rb\"))\n",
    "\n",
    "training_docs = [doc[1] for doc in raw_training_clean]\n",
    "test_docs = [doc[1] for doc in raw_test_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spam_data import training_spam_docs, training_doc_tokens, training_labels, test_labels, test_spam_docs, training_docs, test_docs\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "def create_features_dictionary(document_tokens):\n",
    "    features_dictionary = {}\n",
    "    index = 0\n",
    "    for token in document_tokens:\n",
    "        if token not in features_dictionary:\n",
    "            features_dictionary[token] = index\n",
    "            index += 1\n",
    "    return features_dictionary\n",
    "\n",
    "def tokens_to_bow_vector(document_tokens, features_dictionary):\n",
    "    bow_vector = [0] * len(features_dictionary)\n",
    "    for token in document_tokens:\n",
    "        if token in features_dictionary:\n",
    "            feature_index = features_dictionary[token]\n",
    "            bow_vector[feature_index] += 1\n",
    "    return bow_vector\n",
    "\n",
    "# Define bow_sms_dictionary:\n",
    "bow_sms_dictionary = create_features_dictionary(training_doc_tokens)\n",
    "\n",
    "# Define training_vectors:\n",
    "training_vectors = [tokens_to_bow_vector(token, bow_sms_dictionary) for token in training_spam_docs]\n",
    "\n",
    "# Define test_vectors:\n",
    "test_vectors = [tokens_to_bow_vector(token, bow_sms_dictionary) for token in test_spam_docs]\n",
    "\n",
    "# Machine Learning\n",
    "spam_classifier = MultinomialNB()\n",
    "\n",
    "def spam_or_not(label):\n",
    "    return \"spam\" if label else \"not spam\"\n",
    "\n",
    "# Uncomment the code below when you're done:\n",
    "spam_classifier.fit(training_vectors, training_labels)\n",
    "\n",
    "predictions = spam_classifier.score(test_vectors, test_labels)\n",
    "\n",
    "print(\"The predictions for the test data were {0}% accurate.\\n\\nFor example, '{1}' was classified as {2}.\\n\\nMeanwhile, '{3}' was classified as {4}.\".format(predictions * 100, test_docs[0], spam_or_not(test_labels[0]), test_docs[10], spam_or_not(test_labels[10])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8 - Spam A Lot No More\n",
    "\n",
    "Amazing work! As is the case with many tasks in Python, there’s already a library that can do all of that work for you.\n",
    "\n",
    "For text_to_bow(), you can approximate the functionality with the collections module’s Counter() function:\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "tokens = ['another', 'five', 'fish', 'find', 'another', 'faraway', 'fish']\n",
    "\n",
    "print(Counter(tokens))\n",
    "\n",
    "Counter({'fish': 2, 'another': 2, 'find': 1, 'five': 1, 'faraway': 1})\n",
    "\n",
    "For vectorization, you can use CountVectorizer from the machine learning library scikit-learn. You can use fit() to train the features dictionary and then transform() to transform text into a vector:\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "training_documents = [\"Five fantastic fish flew off to find faraway functions.\", \"Maybe find another five fantastic fish?\", \"Find my fish with a function please!\"]\n",
    "\n",
    "test_text = [\"Another five fish find another faraway fish.\"]\n",
    "\n",
    "bow_vectorizer = CountVectorizer()\n",
    "\n",
    "bow_vectorizer.fit(training_documents)\n",
    "\n",
    "bow_vector = bow_vectorizer.transform(test_text)\n",
    "\n",
    "print(bow_vector.toarray())\n",
    "\n",
    "OP: [[2 0 1 1 2 1 0 0 0 0 0 0 0 0 0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spam_data import training_spam_docs, training_doc_tokens, training_labels, test_labels, test_spam_docs, training_docs, test_docs\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# Import CountVectorizer from sklearn:\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Define bow_vectorizer:\n",
    "bow_vectorizer = CountVectorizer()\n",
    "\n",
    "# fit_transform() does two things: creation of the features dictionary and the vectorization of the training data.\n",
    "\n",
    "# Define training_vectors:\n",
    "training_vectors = bow_vectorizer.fit_transform(training_docs)\n",
    "\n",
    "# Define test_vectors:\n",
    "test_vectors = bow_vectorizer.transform(test_docs)\n",
    "\n",
    "spam_classifier = MultinomialNB()\n",
    "\n",
    "def spam_or_not(label):\n",
    "    return \"spam\" if label else \"not spam\"\n",
    "\n",
    "# Uncomment the code below when you're done:\n",
    "spam_classifier.fit(training_vectors, training_labels)\n",
    "predictions = spam_classifier.score(test_vectors, test_labels)\n",
    "print(\"The predictions for the test data were {0}% accurate.\\n\\nFor example, '{1}' was classified as {2}.\\n\\nMeanwhile, '{3}' was classified as {4}.\".format(predictions * 100, test_docs[7], spam_or_not(test_labels[7]), test_docs[15], spam_or_not(test_labels[15])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9 - BoW Wow\n",
    "\n",
    "As you can see, bag-of-words is pretty useful! BoW also has several advantages over other language models. For one, it’s an easier model to get started with and a few Python libraries already have built-in support for it.\n",
    "\n",
    "Because bag-of-words relies on single words, rather than sequences of words, there are more examples of each unit of language in the training corpus. More examples means the model has less data sparsity (i.e., it has more training knowledge to draw from) than other statistical models.\n",
    "\n",
    "Imagine you want to make a shirt to sell to people. If you have the shirt exactly tailored to someone’s body, it probably won’t fit that many people. But if you make a shirt that is just a giant bag with arm holes, you know that no one will buy it. What do you do? You loosely fit the shirt to someone’s body, leaving some extra room for different body shapes.\n",
    "\n",
    "Overfitting (adapting a model too strongly to training data, akin to our highly tailored shirt) is a common problem for statistical language models. While BoW still suffers from overfitting in terms of vocabulary, it overfits less than other statistical models, allowing for more flexibility in grammar and word choice.\n",
    "\n",
    "The combination of low data sparsity and less overfitting makes the bag-of-words model more reliable with smaller training data sets than other statistical models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Three most frequent word sequences and the number of occurrences according to Bigrams:\n",
      "[(('it', 's'), 1), (('s', 'excite'), 1), (('excite', 'to'), 1)]\n",
      "\n",
      "Three most frequent words and number of occurrences according to Bag-of-Words:\n",
      "[('fish', 4), ('fly', 3), ('day', 3)]\n"
     ]
    }
   ],
   "source": [
    "# The bigrams model is another statistical model that is helpful for tasks like text prediction. However, it’s not always ideal when you want to determine the topic of a given text.\n",
    "\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "text = \"It's exciting to watch flying fish after a hard day's work. I don't know why some fish prefer flying and other fish would rather swim. It seems like the fish just woke up one day and decided, 'hey, today is the day to fly away.'\"\n",
    "tokens = preprocess_text(text)\n",
    "\n",
    "# Bigram approach:\n",
    "bigrams_prepped = ngrams(tokens, 2)\n",
    "bigrams = Counter(bigrams_prepped)\n",
    "print(\"Three most frequent word sequences and the number of occurrences according to Bigrams:\")\n",
    "print(bigrams.most_common(3))\n",
    "\n",
    "# Bag-of-Words approach:\n",
    "# Define bag_of_words here:\n",
    "bag_of_words = Counter(tokens)\n",
    "\n",
    "print(\"\\nThree most frequent words and number of occurrences according to Bag-of-Words:\")\n",
    "\n",
    "most_common_three = bag_of_words.most_common(3)\n",
    "print(most_common_three)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10 - BoW Ow\n",
    "\n",
    "Alas, there is a trade-off for all the brilliance BoW brings to the table.\n",
    "\n",
    "Unless you want sentences that look like “the a but for the”, BoW is NOT a great primary model for text prediction. If that sort of “sentence” isn’t your bag, it’s because bag-of-words has high perplexity, meaning that it’s not a very accurate model for language prediction. The probability of the following word is always just the most frequently used words.\n",
    "\n",
    "If your BoW model finds “good” frequently occurring in a text sample, you might assume there’s a positive sentiment being communicated in that text… but if you look at the original text you may find that in fact every “good” was preceded by a “not.”\n",
    "\n",
    "Hmm, that would have been helpful to know. The BoW model’s word tokens lack context, which can make a word’s intended meaning unclear.\n",
    "\n",
    "Perhaps you are wondering, “What happens if the model comes across a new word that wasn’t in the training data?” As mentioned, like all statistical models, BoW suffers from overfitting when it comes to vocabulary.\n",
    "\n",
    "There are several ways that NLP developers have tackled this issue. A common approach is through language smoothing in which some probability is siphoned from the known words and given to unknown words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in white leaning over the ordinary jug or purposeless day or strange about it should be made clear i think that are dancing on but i did not thrive without such men say that old offender the simplest of all the movement which is not given to his work done\n"
     ]
    }
   ],
   "source": [
    "import nltk, re, random\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict, deque, Counter\n",
    "from document import oscar_wilde_thoughts\n",
    "\n",
    "# Change sequence_length:\n",
    "sequence_length = 3\n",
    "\n",
    "class MarkovChain:\n",
    "  def __init__(self):\n",
    "    self.lookup_dict = defaultdict(list)\n",
    "    self.most_common = []\n",
    "    self._seeded = False\n",
    "    self.__seed_me()\n",
    "\n",
    "  def __seed_me(self, rand_seed=None):\n",
    "    if self._seeded is not True:\n",
    "      try:\n",
    "        if rand_seed is not None:\n",
    "          random.seed(rand_seed)\n",
    "        else:\n",
    "          random.seed()\n",
    "        self._seeded = True\n",
    "      except NotImplementedError:\n",
    "        self._seeded = False\n",
    "    \n",
    "  def add_document(self, str):\n",
    "    preprocessed_list = self._preprocess(str)\n",
    "    self.most_common = Counter(preprocessed_list).most_common(50)\n",
    "    pairs = self.__generate_tuple_keys(preprocessed_list)\n",
    "    for pair in pairs:\n",
    "      self.lookup_dict[pair[0]].append(pair[1])\n",
    "  \n",
    "  def _preprocess(self, str):\n",
    "    cleaned = re.sub(r'\\W+', ' ', str).lower()\n",
    "    tokenized = word_tokenize(cleaned)\n",
    "    return tokenized\n",
    "\n",
    "  def __generate_tuple_keys(self, data):\n",
    "    if len(data) < sequence_length:\n",
    "      return\n",
    "\n",
    "    for i in range(len(data) - 1):\n",
    "      yield [ data[i], data[i + 1] ]\n",
    "      \n",
    "  def generate_text(self, max_length=50):\n",
    "    context = deque()\n",
    "    output = []\n",
    "    if len(self.lookup_dict) > 0:\n",
    "      self.__seed_me(rand_seed=len(self.lookup_dict))\n",
    "      chain_head = [list(self.lookup_dict)[0]]\n",
    "      context.extend(chain_head)\n",
    "      if sequence_length > 1:\n",
    "        while len(output) < (max_length - 1):\n",
    "          next_choices = self.lookup_dict[context[-1]]\n",
    "          if len(next_choices) > 0:\n",
    "            next_word = random.choice(next_choices)\n",
    "            context.append(next_word)\n",
    "            output.append(context.popleft())\n",
    "          else:\n",
    "            break\n",
    "        output.extend(list(context))\n",
    "      else:\n",
    "        while len(output) < (max_length - 1):\n",
    "          next_choices = [word[0] for word in self.most_common]\n",
    "          next_word = random.choice(next_choices)\n",
    "          output.append(next_word)\n",
    "    return \" \".join(output)\n",
    "\n",
    "my_markov = MarkovChain()\n",
    "my_markov.add_document(oscar_wilde_thoughts)\n",
    "random_oscar_wilde = my_markov.generate_text()\n",
    "print(random_oscar_wilde)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review of Bag-of-Words\n",
    "\n",
    "- Bag-of-words (BoW) — also referred to as the unigram model — is a statistical language model based on word count.\n",
    "- There are loads of real-world applications for BoW.\n",
    "- BoW can be implemented as a Python dictionary with each key set to a word and each value set to the number of times that word appears in a text.\n",
    "- For BoW, training data is the text that is used to build a BoW model.\n",
    "- BoW test data is the new text that is converted to a BoW vector using a trained features dictionary.\n",
    "- A feature vector is a numeric depiction of an item’s salient features.\n",
    "- Feature extraction (or vectorization) is the process of turning text into a BoW vector.\n",
    "- A features dictionary is a mapping of each unique word in the training data to a unique index. This is used to build out BoW vectors.\n",
    "- BoW has less data sparsity than other statistical models. It also suffers less from overfitting.\n",
    "- BoW has higher perplexity than other models, making it less ideal for language prediction.\n",
    "- One solution to overfitting is language smoothing, in which a bit of probability is taken from known words and allotted to unknown words.\n",
    "\n",
    "The spam data for this lesson were taken from the UCI Machine Learning Repository.\n",
    "\n",
    "Dua, D. and Karra Taniskidou, E. (2017). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quiz:\n",
    "\n",
    "- Language smoothing is a technique used across many statistical models. BoW has less data sparsity because there are more examples of each sequence in the training data.\n",
    "- BoW is concerned with how often each word appears in a given piece of text."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
