{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Discovering new code words in declassified CIA documents may seem like a mission for a foreign intelligence service, and detecting [gender biases](https://medium.com/agatha-codes/a-bossy-sort-of-voice-3c3a18de3093) in the Harry Potter novels a task for a literature professor. Yet by utilizing natural language parsing with regular expressions, the power to perform such analyses is in your own hands!\n",
    "\n",
    "While you may not put much explicit thought into the structure of your sentences as you write, the syntax choices you make are critical in ensuring your writing has meaning. Analyzing such sentence structure as well as word choice can not only provide insights into the connotation of a piece text, but can also highlight the biases of its author or uncover additional insights that even a [deep, rigorous reading of the text might not reveal.](https://twitter.com/hexadecim8/status/1068215227274137605)\n",
    "\n",
    "By using Python’s regular expression modulere and the Natural Language Toolkit, known as NLTK, you can find keywords of interest, discover where and how often they are used, and discern the parts-of-speech patterns in which they appear to understand the sometimes hidden meaning in a piece of writing. Let’s get started!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Chunk Counter\n",
    "from collections import Counter\n",
    "\n",
    "def np_chunk_counter(chunked_sentences):\n",
    "\n",
    "    # create a list to hold chunks\n",
    "    chunks = list()\n",
    "\n",
    "    # for-loop through each chunked sentence to extract noun phrase chunks\n",
    "    for chunked_sentence in chunked_sentences:\n",
    "        for subtree in chunked_sentence.subtrees(filter=lambda t: t.label() == 'NP'):\n",
    "            chunks.append(tuple(subtree))\n",
    "\n",
    "    # create a Counter object\n",
    "    chunk_counter = Counter()\n",
    "\n",
    "    # for-loop through the list of chunks\n",
    "    for chunk in chunks:\n",
    "        # increase counter of specific chunk by 1\n",
    "        chunk_counter[chunk] += 1\n",
    "\n",
    "    # return 30 most frequent chunks\n",
    "    return chunk_counter.most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((('i', 'NN'),), 326), ((('dorothy', 'NN'),), 222), ((('the', 'DT'), ('scarecrow', 'NN')), 213), ((('the', 'DT'), ('lion', 'NN')), 148), ((('the', 'DT'), ('tin', 'NN')), 123), ((('woodman', 'NN'),), 112), ((('oz', 'NN'),), 86), ((('toto', 'NN'),), 73), ((('head', 'NN'),), 59), ((('the', 'DT'), ('woodman', 'NN')), 59), ((('the', 'DT'), ('wicked', 'JJ'), ('witch', 'NN')), 58), ((('the', 'DT'), ('emerald', 'JJ'), ('city', 'NN')), 51), ((('the', 'DT'), ('witch', 'NN')), 49), ((('the', 'DT'), ('girl', 'NN')), 46), ((('the', 'DT'), ('road', 'NN')), 41), ((('room', 'NN'),), 29), ((('nothing', 'NN'),), 29), ((('the', 'DT'), ('air', 'NN')), 29), ((('the', 'DT'), ('country', 'NN')), 26), ((('the', 'DT'), ('land', 'NN')), 24), ((('a', 'DT'), ('heart', 'NN')), 24), ((('the', 'DT'), ('west', 'NN')), 23), ((('axe', 'NN'),), 23), ((('the', 'DT'), ('sun', 'NN')), 22), ((('the', 'DT'), ('little', 'JJ'), ('girl', 'NN')), 22), ((('course', 'NN'),), 22), ((('the', 'DT'), ('cowardly', 'JJ'), ('lion', 'NN')), 21), ((('aunt', 'NN'),), 21), ((('the', 'DT'), ('house', 'NN')), 21), ((('the', 'DT'), ('door', 'NN')), 21)]\n"
     ]
    }
   ],
   "source": [
    "# Chunking\n",
    "\n",
    "from nltk import RegexpParser\n",
    "from pos_tagged_oz import pos_tagged_oz\n",
    "\n",
    "# define noun-phrase chunk grammar here\n",
    "chunk_grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "\n",
    "# create RegexpParser object here\n",
    "chunk_parser = RegexpParser(chunk_grammar)\n",
    "\n",
    "# create a list to hold noun-phrase chunked sentences\n",
    "np_chunked_oz = list()\n",
    "\n",
    "# create a for-loop through each pos-tagged sentence in pos_tagged_oz here\n",
    "for pos_tagged_sentence in pos_tagged_oz:\n",
    "  # chunk each sentence and append to np_chunked_oz here\n",
    "  np_chunked_oz.append(chunk_parser.parse(pos_tagged_sentence))\n",
    "\n",
    "# store and print the most common np-chunks here\n",
    "most_common_np_chunks = np_chunk_counter(np_chunked_oz)\n",
    "print(most_common_np_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Compiling and Matching\n",
    "\n",
    "Before you dive into more complex syntax parsing, you’ll begin with basic regular expressions in Python using the re module as a regex refresher.\n",
    "\n",
    "The first method you will explore is .compile(). This method takes a regular expression pattern as an argument and compiles the pattern into a regular expression object, which you can later use to find matching text. The regular expression object below will exactly match 4 upper or lower case characters.\n",
    "\n",
    "regular_expression_object = re.compile(\"[A-Za-z]{4}\")\n",
    "\n",
    "Regular expression objects have a .match() method that takes a string of text as an argument and looks for a single match to the regular expression that starts at the beginning of the string. To see if your regular expression matches the string \"Toto\" you can do the following:\n",
    "\n",
    "result = regular_expression_object.match(\"Toto\")\n",
    "\n",
    "If .match() finds a match that starts at the beginning of the string, it will return a match object. The match object lets you know what piece of text the regular expression matched, and at what index the match begins and ends. If there is no match, .match() will return None.\n",
    "\n",
    "With the match object stored in result, you can access the matched text by calling result.group(0). If you use a regex containing capture groups, you can access these groups by calling .group() with the appropriately numbered capture group as an argument.\n",
    "\n",
    "Instead of compiling the regular expression first and then looking for a match in separate lines of code, you can simplify your match to one line:\n",
    "\n",
    "result = re.match(\"[A-Za-z]{4}\",\"Toto\")\n",
    "\n",
    "With this syntax, re‘s .match() method takes a regular expression pattern as the first argument and a string as the second argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_sre.SRE_Match object; span=(0, 7), match='Dorothy'>\n",
      "Dorothy\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 1 - Compiling and Matching\n",
    "\n",
    "import re\n",
    "\n",
    "# characters are defined\n",
    "character_1 = \"Dorothy\"\n",
    "character_2 = \"Henry\"\n",
    "\n",
    "# compile your regular expression here\n",
    "regular_expression = re.compile(\"\\w{7}\")\n",
    "\n",
    "# check for a match to character_1 here\n",
    "result_1 = regular_expression.match(character_1)\n",
    "print(result_1)\n",
    "\n",
    "# store and print the matched text here\n",
    "match_1 = result_1.group(0)\n",
    "print(match_1)\n",
    "\n",
    "# compile a regular expression to match a 7 character string of word characters and check for a match to character_2 here\n",
    "result_2 = re.match(\"\\w{7}\",character_2)\n",
    "print(result_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Searching and Finding\n",
    "\n",
    "You can make your regular expression matches even more dynamic with the help of the .search() method. Unlike .match() which will only find matches at the start of a string, .search() will look left to right through an entire piece of text and return a match object for the first match to the regular expression given. If no match is found, .search() will return None. For example, to search for a sequence of 8 word characters in the string Are you a Munchkin?:\n",
    "\n",
    "result = re.search(\"\\w{8}\",\"Are you a Munchkin?\")\n",
    "\n",
    "Using .search() on the string above will find a match of \"Munchkin\", while using .match() on the same string would return None!\n",
    "\n",
    "So far you have used methods that only return one piece of matching text. What if you want to find all the occurrences of a word or keyword in a piece of text to determine a frequency count? Step in the .findall() method!\n",
    "\n",
    "Given a regular expression as its first argument and a string as its second argument, .findall() will return a list of all non-overlapping matches of the regular expression in the string. Consider the below piece of text:\n",
    "\n",
    "text = \"Everything is green here, while in the country of the Munchkins blue was the favorite color. But the people do not seem to be as friendly as the Munchkins, and I'm afraid we shall be unable to find a place to pass the night.\"\n",
    "\n",
    "To find all non-overlapping sequences of 8 word characters in the sentence you can do the following:\n",
    "\n",
    "list_of_matches = re.findall(\"\\w{8}\",text)\n",
    "\n",
    ".findall() will thus return the list ['Everythi', 'Munchkin', 'favorite', 'friendly', 'Munchkin']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_sre.SRE_Match object; span=(1528, 1533), match='Baker'>\n",
      "['Holmes', 'Holmes', 'Holmes', 'Holmes', 'Holmes', 'Holmes', 'Holmes', 'Holmes', 'Holmes', 'Holmes', 'Holmes', 'Holmes', 'Holmes', 'Holmes', 'Holmes', 'Holmes', 'Holmes', 'Holmes', 'Holmes', 'Holmes', 'Holmes', 'Holmes', 'Holmes', 'Holmes']\n",
      "24\n",
      "------------------------------\n",
      "def np_chunk_counter(chunked_sentences):\n",
      "\n",
      "    # create a list to hold chunks\n",
      "    chunks = list()\n",
      "\n",
      "    # for-loop through each chunked sentence to extract noun phrase chunks\n",
      "    for chunked_sentence in chunked_sentences:\n",
      "        for subtree in chunked_sentence.subtrees(filter=lambda t: t.label() == 'NP'):\n",
      "            chunks.append(tuple(subtree))\n",
      "\n",
      "    # create a Counter object\n",
      "    chunk_counter = Counter()\n",
      "\n",
      "    # for-loop through the list of chunks\n",
      "    for chunk in chunks:\n",
      "        # increase counter of specific chunk by 1\n",
      "        chunk_counter[chunk] += 1\n",
      "\n",
      "    # return 30 most frequent chunks\n",
      "    return chunk_counter.most_common(30)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Searching and Finding\n",
    "\n",
    "import re\n",
    "from sherlock_holmes import bohemia_ch1\n",
    "\n",
    "# import L. Frank Baum's The Wonderful Wizard of Oz\n",
    "#oz_text = open(\"the_wizard_of_oz_text.txt\",encoding='utf-8').read().lower()\n",
    "\n",
    "oz_text = bohemia_ch1\n",
    "\n",
    "# search oz_text for an occurrence of 'Baker' here\n",
    "found_Baker = re.search(\"Baker\", oz_text)\n",
    "print(found_Baker)\n",
    "\n",
    "# find all the occurrences of 'Holmes' in oz_text here\n",
    "all_Holmes = re.findall(\"Holmes\", oz_text)\n",
    "print(all_Holmes)\n",
    "\n",
    "# store and print the length of all_lions here\n",
    "number_Holmes = len(all_Holmes)\n",
    "print(number_Holmes)\n",
    "\n",
    "print('-'*30)\n",
    "# To Inspect a Function, use\n",
    "\n",
    "import inspect\n",
    "print(inspect.getsource(np_chunk_counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Part-of-Speech Tagging\n",
    "\n",
    "While it is useful to match and search for patterns of individual characters in a text, you can often find more meaning by analyzing text on a word-by-word basis, focusing on the part of speech of each word in a sentence. This process of identifying and labeling the part of speech of words is known as part-of-speech tagging!\n",
    "\n",
    "It may have been a while since you’ve been in English class, so let’s review the nine parts of speech with an example:\n",
    "\n",
    "Wow! Ramona and her class are happily studying the new textbook she has on NLP.\n",
    "\n",
    "- Noun: the name of a person (Ramona,class), place, thing (textbook), or idea (NLP)\n",
    "- Pronoun: a word used in place of a noun (her,she)\n",
    "- Determiner: a word that introduces, or “determines”, a noun (the)\n",
    "- Verb: expresses action (studying) or being (are,has)\n",
    "- Adjective: modifies or describes a noun or pronoun (new)\n",
    "- Adverb: modifies or describes a verb, an adjective, or another adverb (happily)\n",
    "- Preposition: a word placed before a noun or pronoun to form a phrase modifying another word in the sentence (on)\n",
    "- Conjunction: a word that joins words, phrases, or clauses (and)\n",
    "- Interjection: a word used to express emotion (Wow)\n",
    "\n",
    "You can automate the part-of-speech tagging process with nltk‘s pos_tag() function! The function takes one argument, a list of words in the order they appear in a sentence, and returns a list of tuples, where the first entry in the tuple is a word and the second is the part-of-speech tag.\n",
    "\n",
    "Given the sentence split into a list of words below:\n",
    "\n",
    "word_sentence = ['do', 'you', 'suppose', 'oz', 'could', 'give', 'me', 'a', 'heart', '?']<br>\n",
    "\n",
    "you can tag the parts of speech as follows:\n",
    "\n",
    "part_of_speech_tagged_sentence = pos_tag(word_sentence) <br>\n",
    "The call to pos_tag() will return the following:\n",
    "\n",
    "[('do', 'VB'), ('you', 'PRP'), ('suppose', 'VB'), ('oz', 'NNS'), ('could', 'MD'), ('give', 'VB'), ('me', 'PRP'), ('a', 'DT'), ('heart', 'NN'), ('?', '.')]\n",
    "\n",
    "Abbreviations are given instead of the full part of speech name. Some common abbreviations include: NN for nouns, VB for verbs, RB for adverbs, JJ for adjectives, and DT for determiners. [A complete list of part-of-speech tags and their abbreviations can be found here.](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['``', 'the', 'house', 'must', 'have', 'fallen', 'on', 'her', '.']\n",
      "[('``', '``'), ('the', 'DT'), ('house', 'NN'), ('must', 'MD'), ('have', 'VB'), ('fallen', 'VBN'), ('on', 'IN'), ('her', 'PRP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# 4 - POS Tagginf\n",
    "\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from word_tokenized_oz import word_tokenized_oz # Already Sentence Tokenized\n",
    "\n",
    "# save and print the sentence stored at index 100 in word_tokenized_oz here\n",
    "\n",
    "witches_fate = word_tokenized_oz[100]\n",
    "print(witches_fate)\n",
    "# ['``', 'the', 'house', 'must', 'have', 'fallen', 'on', 'her', '.']\n",
    "\n",
    "# create a list to hold part-of-speech tagged sentences here\n",
    "pos_tagged_oz  = list()\n",
    "\n",
    "# create a for loop through each word tokenized sentence in word_tokenized_oz here\n",
    "for text in word_tokenized_oz:\n",
    "  # part-of-speech tag each sentence and append to pos_tagged_oz here\n",
    "  pos_tagged_oz.append(pos_tag(text))\n",
    "\n",
    "# store and print the 101st part-of-speech tagged sentence here\n",
    "witches_fate_pos = pos_tagged_oz[100]\n",
    "# [('``', '``'), ('the', 'DT'), ('house', 'NN'), ('must', 'MD'), ('have', 'VB'), ('fallen', 'VBN'), ('on', 'IN'), ('her', 'PRP'), ('.', '.')]\n",
    "\n",
    "print(witches_fate_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
