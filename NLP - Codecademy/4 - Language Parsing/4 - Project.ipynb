{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discover Insights into Classic Texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Novels and text contain insights into ideologies and places that are often originally unknown to the reader. By reading a written piece, you uncover the opinions of the author on their chosen topic and come to understand both the topic and how the author thinks.\n",
    "\n",
    "In this project you will perform a natural language parsing analysis to gain deeper insight into one of two famous and often discussed novels in the public domain: [Oscar Wilde’s The Picture of Dorian Gray](http://www.gutenberg.org/ebooks/174) or [Homer’s The Iliad!](http://www.gutenberg.org/ebooks/6130) Fear not if you haven’t heard or read the novels, one of the beauties of natural language parsing with regular expressions is the ability to gain insight into lengthy pieces of text without a formal read!\n",
    "\n",
    "By the end of this project, you will find out the main topics of discussion in the novel of your choosing and can begin to discern some of the author’s thoughts and beliefs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Tokenized Words\n",
    "\n",
    "from nltk.tokenize import PunktSentenceTokenizer, word_tokenize\n",
    "\n",
    "def word_sentence_tokenize(text):\n",
    "  \n",
    "    # create a PunktSentenceTokenizer\n",
    "    sentence_tokenizer = PunktSentenceTokenizer(text)\n",
    "\n",
    "    # sentence tokenize text\n",
    "    sentence_tokenized = sentence_tokenizer.tokenize(text)\n",
    "\n",
    "    # create a list to hold word tokenized sentences\n",
    "    word_tokenized = list()\n",
    "\n",
    "    # for-loop through each tokenized sentence in sentence_tokenized\n",
    "    for tokenized_sentence in sentence_tokenized:\n",
    "    # word tokenize each sentence and append to word_tokenized\n",
    "        word_tokenized.append(word_tokenize(tokenized_sentence))\n",
    "\n",
    "    return word_tokenized\n",
    "    \n",
    "# Define Chunking Counters\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# function that pulls chunks out of chunked sentence and finds the most common chunks\n",
    "def np_chunk_counter(chunked_sentences):\n",
    "\n",
    "    # create a list to hold chunks\n",
    "    chunks = list()\n",
    "\n",
    "    # for-loop through each chunked sentence to extract noun phrase chunks\n",
    "    for chunked_sentence in chunked_sentences:\n",
    "        for subtree in chunked_sentence.subtrees(filter=lambda t: t.label() == 'NP'):\n",
    "            chunks.append(tuple(subtree))\n",
    "\n",
    "    # create a Counter object\n",
    "    chunk_counter = Counter()\n",
    "\n",
    "    # for-loop through the list of chunks\n",
    "    for chunk in chunks:\n",
    "        # increase counter of specific chunk by 1\n",
    "        chunk_counter[chunk] += 1\n",
    "\n",
    "    # return 30 most frequent chunks\n",
    "    return chunk_counter.most_common(30)\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# function that pulls chunks out of chunked sentence and finds the most common chunks\n",
    "def vp_chunk_counter(chunked_sentences):\n",
    "\n",
    "    # create a list to hold chunks\n",
    "    chunks = list()\n",
    "\n",
    "    # for-loop through each chunked sentence to extract verb phrase chunks\n",
    "    for chunked_sentence in chunked_sentences:\n",
    "        for subtree in chunked_sentence.subtrees(filter=lambda t: t.label() == 'VP'):\n",
    "            chunks.append(tuple(subtree))\n",
    "\n",
    "    # create a Counter object\n",
    "    chunk_counter = Counter()\n",
    "\n",
    "    # for-loop through the list of chunks\n",
    "    for chunk in chunks:\n",
    "        # increase counter of specific chunk by 1\n",
    "        chunk_counter[chunk] += 1\n",
    "\n",
    "    # return 30 most frequent chunks\n",
    "    return chunk_counter.most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Single word Token Sentence is: \n",
      "\n",
      "['illustrations', 'homer', 'invoking', 'the', 'muse', '.']\n",
      "\n",
      "\n",
      "The part-of-speech tagged sentence for 100th index is: \n",
      "\n",
      "[('he', 'PRP'), ('appears', 'VBZ'), ('as', 'IN'), ('the', 'DT'), ('enunciator', 'NN'), ('of', 'IN'), ('opinions', 'NNS'), ('as', 'IN'), ('different', 'JJ'), ('in', 'IN'), ('their', 'PRP$'), ('tone', 'NN'), ('as', 'IN'), ('those', 'DT'), ('of', 'IN'), ('the', 'DT'), ('writers', 'NNS'), ('who', 'WP'), ('have', 'VBP'), ('handed', 'VBN'), ('them', 'PRP'), ('down', 'RP'), ('.', '.')]\n",
      "\n",
      "\n",
      "The most common NP-chunks here are: \n",
      "\n",
      "[((('hector', 'NN'),), 322), ((('i', 'NN'),), 277), ((('jove', 'NN'),), 257), ((('troy', 'NN'),), 208), ((('vain', 'NN'),), 195), ((('war', 'NN'),), 193), ((('son', 'NN'),), 170), ((('thou', 'NN'),), 158), ((('the', 'DT'), ('plain', 'NN')), 157), ((('the', 'DT'), ('field', 'NN')), 154), ((('the', 'DT'), ('ground', 'NN')), 138), ((('death', 'NN'),), 134), ((('hand', 'NN'),), 134), ((('greece', 'NN'),), 128), ((('heaven', 'NN'),), 127), ((('fate', 'NN'),), 127), ((('thee', 'NN'),), 122), ((('breast', 'NN'),), 121), ((('the', 'DT'), ('trojan', 'NN')), 120), ((('the', 'DT'), ('god', 'NN')), 119), ((('the', 'DT'), ('war', 'NN')), 117), ((('the', 'DT'), ('greeks', 'NN')), 116), ((('blood', 'NN'),), 115), ((('homer', 'NN'),), 112), ((('the', 'DT'), ('king', 'NN')), 105), ((('rage', 'NN'),), 103), ((('force', 'NN'),), 103), ((('care', 'NN'),), 99), ((('head', 'NN'),), 98), ((('man', 'NN'),), 97)]\n",
      "\n",
      "\n",
      "The most common VP-chunks here are: \n",
      "\n",
      "[(((\"'t\", 'NN'), ('is', 'VBZ')), 19), ((('i', 'NN'), ('am', 'VBP')), 11), (((\"'t\", 'NN'), ('was', 'VBD')), 11), ((('the', 'DT'), ('hero', 'NN'), ('said', 'VBD')), 9), ((('i', 'NN'), ('know', 'VBP')), 8), ((('i', 'NN'), ('saw', 'VBD')), 8), ((('the', 'DT'), ('scene', 'NN'), ('lies', 'VBZ')), 7), ((('i', 'NN'), ('was', 'VBD')), 6), ((('confess', 'NN'), (\"'d\", 'VBD')), 6), ((('the', 'DT'), ('scene', 'NN'), ('is', 'VBZ')), 6), ((('view', 'NN'), (\"'d\", 'VBD')), 5), ((('i', 'NN'), ('felt', 'VBD')), 5), ((('i', 'NN'), ('bear', 'VBP')), 5), ((('hector', 'NN'), ('is', 'VBZ')), 5), ((('vain', 'NN'), ('was', 'VBD')), 5), ((('homer', 'NN'), ('was', 'VBD')), 4), ((('i', 'NN'), ('have', 'VBP')), 4), ((('hunger', 'NN'), ('was', 'VBD')), 4), ((('glory', 'NN'), ('lost', 'VBN')), 4), ((('i', 'NN'), ('see', 'VBP')), 4), ((('war', 'NN'), ('be', 'VB')), 4), ((('the', 'DT'), ('weapon', 'NN'), ('stood', 'VBD')), 4), ((('i', 'NN'), ('go', 'VBP')), 4), ((('the', 'DT'), ('silence', 'NN'), ('broke', 'VBD')), 4), ((('the', 'DT'), ('trojan', 'NN'), ('bands', 'VBZ')), 4), ((('father', 'NN'), ('gave', 'VBD')), 4), ((('i', 'NN'), ('deem', 'VBP')), 4), ((('minerva', 'NN'), ('repressing', 'VBG')), 3), ((('thetis', 'NN'), ('calling', 'VBG')), 3), ((('thetis', 'NN'), ('entreating', 'VBG')), 3)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Identifying the Most Common Word Occurances\n",
    "\n",
    "from nltk import pos_tag, RegexpParser\n",
    "\n",
    "# import text of choice here\n",
    "text = open(\"the_iliad.txt\",encoding='utf-8').read().lower()\n",
    "\n",
    "# sentence and word tokenize text here\n",
    "word_tokenized_text = word_sentence_tokenize(text)\n",
    "\n",
    "# store and print any word tokenized sentence here\n",
    "single_word_tokenized_sentence = word_tokenized_text[10]\n",
    "print(\"The Single word Token Sentence is: \\n\")\n",
    "print(single_word_tokenized_sentence)\n",
    "print('\\n')\n",
    "\n",
    "# create a list to hold part-of-speech tagged sentences here\n",
    "pos_tagged_text = list()\n",
    "\n",
    "# create a for loop through each word tokenized sentence here\n",
    "for token in word_tokenized_text:\n",
    "  # part-of-speech tag each sentence and append to list of pos-tagged sentences here\n",
    "    pos_tagged_text.append(pos_tag(token))\n",
    "\n",
    "# store and print any part-of-speech tagged sentence here\n",
    "single_pos_sentence = pos_tagged_text[100]\n",
    "print(\"The part-of-speech tagged sentence for 100th index is: \\n\")\n",
    "print(pos_tagged_text[100])\n",
    "print('\\n')\n",
    "\n",
    "# define noun phrase chunk grammar here\n",
    "np_chunk_grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "\n",
    "# create noun phrase RegexpParser object here\n",
    "np_chunk_parser = RegexpParser(np_chunk_grammar)\n",
    "\n",
    "# define verb phrase chunk grammar here\n",
    "vp_chunk_grammar = \"VP: {<DT>?<JJ>*<NN><VB.*><RB.?>?}\"\n",
    "\n",
    "# create verb phrase RegexpParser object here\n",
    "vp_chunk_parser = RegexpParser(vp_chunk_grammar)\n",
    "\n",
    "# create a list to hold noun phrase chunked sentences and a list to hold verb phrase chunked sentences here\n",
    "np_chunked_text = list()\n",
    "vp_chunked_text = list()\n",
    "\n",
    "# create a for loop through each pos-tagged sentence here\n",
    "for token in pos_tagged_text:\n",
    "  # chunk each sentence and append to lists here\n",
    "    np_chunked_text.append(np_chunk_parser.parse(token))\n",
    "    vp_chunked_text.append(vp_chunk_parser.parse(token))  \n",
    "\n",
    "# store and print the most common NP-chunks here\n",
    "most_common_np_chunks = np_chunk_counter(np_chunked_text)\n",
    "print(\"The most common NP-chunks here are: \\n\")\n",
    "print(most_common_np_chunks)\n",
    "print('\\n')\n",
    "\n",
    "# store and print the most common VP-chunks here\n",
    "most_common_vp_chunks = vp_chunk_counter(vp_chunked_text)\n",
    "print(\"The most common VP-chunks here are: \\n\")\n",
    "print(most_common_vp_chunks)\n",
    "print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
